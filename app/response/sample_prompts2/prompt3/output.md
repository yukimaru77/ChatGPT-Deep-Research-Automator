了解しました。第7章～第9章（リカレントニューラルネットワーク、自然言語処理の基礎、アテンションメカニズム）について、情報系大学院生向けに専門的かつ丁寧な解説資料を日本語で作成します。

各章には図や数式、コード例を用い、複数の信頼性ある日本語・英語・中国語文献を統合して、最高の内容を目指します。画像は引用元を明記しつつ適切に使用し、読者の理解を深める資料を仕上げます。

資料がまとまり次第、マークダウン形式でお届けします。しばらくお待ちください。

## 第7章：リカレントニューラルネットワーク（RNN）
リカレントニューラルネットワーク（**RNN**）は、時系列データや系列データを扱うために設計されたニューラルネットワークです。通常のフィードフォワード型ニューラルネットワークとは異なり、RNNは**「隠れ状態」**(hidden state)と呼ばれる内部メモリを持ち、これにより**過去の入力の情報**を保持しつつ現在の出力に反映することができます ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=,During%20training%20via%20backpropagation%20through))。具体的には、RNNでは時刻$t$の隠れ状態$\mathbf{h}_t$が、前時刻の隠れ状態$\mathbf{h}_{t-1}$と現在の入力$\mathbf{x}_t$に依存して計算されます。そのため、シーケンス（系列）上で**同じネットワークを繰り返し適用**していく構造になっており、時間的に展開（アンロール）して見ると非常に深いネットワークとみなせます ([Visualizations of RNN units - Jakub Kvita](https://kvitajakub.github.io/2016/04/14/rnn-diagrams/#:~:text=I%20am%20currently%20writing%20my,knowledge%20about%20RNNs%20and%20LSTMs)) ([Visualizations of RNN units - Jakub Kvita](https://kvitajakub.github.io/2016/04/14/rnn-diagrams/#:~:text=Image%3A%20RNN%20Unrolling%20image%20RNN,network%20very%20deep%20in%20time))（図7.1）。形式的には、単純なRNNセルの再帰関係は例えば次のように表せます：

$$
\mathbf{h}_t = \tanh(W_x \mathbf{x}_t + W_h \mathbf{h}_{t-1} + \mathbf{b}),
$$

ここで$W_x$, $W_h$, $\mathbf{b}$はそれぞれ入力に対する重み、隠れ状態に対する再帰的重み、バイアス項を表します（活性化関数には一般に$tanh$や$ReLU$が用いられます）。また出力$\mathbf{y}_t$が必要な場合は、$\mathbf{h}_t$にさらに重み行列$W_y$をかけるなどして計算します。RNNでは全時刻でこれらの重みが**共有**されるため、パラメータ数を抑えつつ系列長に制限なく処理が可能です。

 ([Visualizations of RNN units - Jakub Kvita](https://kvitajakub.github.io/2016/04/14/rnn-diagrams/))図7.1: RNNの内部構造の展開図。左の図はRNNセルAを用いた閉路構造を示し、右の図は時間方向に展開（アンロール）した計算グラフを示す ([Visualizations of RNN units - Jakub Kvita](https://kvitajakub.github.io/2016/04/14/rnn-diagrams/#:~:text=I%20am%20currently%20writing%20my,knowledge%20about%20RNNs%20and%20LSTMs)) ([Visualizations of RNN units - Jakub Kvita](https://kvitajakub.github.io/2016/04/14/rnn-diagrams/#:~:text=Image%3A%20RNN%20Unrolling%20image%20RNN,network%20very%20deep%20in%20time))。各時刻で同じネットワーク（重み）Aが用いられ、隠れ状態$h_t$が次の時刻へ伝播される。

### BPTTと勾配消失・爆発問題
RNNの学習は**時間方向に展開したネットワーク**に対して誤差逆伝搬法（**Backpropagation Through Time**, BPTT）を適用することで行われます ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=))。すなわち、シーケンスの各時刻で算出された損失を合計し（あるいは系列全体での損失を定義し）、それを各時刻の重み（共有されている）に対して微分することで勾配を計算します ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=To%20understand%20and%20visualize%20the,is%20called%20Backpropogate%20through%20time)) ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=While%20you%20are%20using%20Backpropogating,is%20very%20less%20than%201))。この際、時間方向に**非常に深いネットワーク**になるため、誤差勾配が過去に伝わるにつれて指数的に小さくなったり大きくなったりする問題が発生します。前者を**勾配消失**（vanishing gradient）、後者を**勾配爆発**（exploding gradient）と呼びます ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=While%20Backpropogating%20you%20may%20get,2%20types%20of%20issues)) ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=))。特にRNNでは勾配消失により**長期の依存関係（長い時系列の依存）を学習できない**という問題が指摘されました ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=Vanishing%20Gradient%3A%20where%20the%20contribution,for%20the%20vanilla%20RNN%20unit)) ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=theoretically%20capable%20of%20handling%20long,regulate%20the%20flow%20of%20information))。例えば、ある時刻$t$の出力に数百ステップも前の入力が影響を与えるような関係（長期依存）は、勾配消失により訓練が困難になります。

勾配消失のメカニズムを簡単に説明します。各時間ステップでの誤差の勾配はチェインルールによって過去のステップへ伝播しますが、その際に隠れ状態の再帰部分の微分（$\partial \mathbf{h}_t/\partial \mathbf{h}_{t-1}$）が連鎖的に掛け合わされていきます ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=While%20you%20are%20using%20Backpropogating,is%20very%20less%20than%201))。この微分は、例えば活性化関数がtanhの場合最大でも1未満の値をとるため（また重み行列のノルムも影響します）、何百回も掛け合わせると$0$に近づいてしまいます ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=If%20the%20partial%20derivation%20of,when%20compared%20with%20previous%20iteration))。その結果、序盤のステップの重みまで勾配が届かず**「昔の情報」ほど学習されない**状態になります ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=you%20can%20see%20that%20there,Gradience%20is%20called%20Vanishing%20Gradience))。逆に再帰の勾配が1を超える場合には指数的に発散し、勾配爆発が生じます ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=))。勾配爆発は学習を不安定にしますが、これは**勾配クリッピング**（ある閾値以上の勾配を一定値に抑制する）など比較的簡単な対策で解決できます ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=How%20can%20you%20overcome%20the,of%20Vanishing%20and%20Exploding%20Gradience))。一方、勾配消失への対策はもう少し工夫が必要です。

**勾配消失問題への主な対策**として、以下のような手法が知られています ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=How%20can%20you%20overcome%20the,of%20Vanishing%20and%20Exploding%20Gradience))：

- **活性化関数の工夫**：ReLUのように勾配が消えにくい関数を使う（但しReLUにも別の問題があります）。
- **トランケートBPTT**：一定時間ステップ以上は逆伝搬を行わないことで、過去に遡りすぎて勾配が極端に小さくなるのを避ける ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=1,overcome%20with))。
- **ゲート付きRNNの利用**：以下で述べるLSTMやGRUといったアーキテクチャで、勾配が流れやすい設計を使う ([Recurrent Neural Networks and LSTM explained - Mohamed DHAOUI](https://mohameddhaoui.github.io/deeplearning/LSTM/#:~:text=How%20can%20you%20overcome%20the,of%20Vanishing%20and%20Exploding%20Gradience))。

### LSTM: 長短期記憶ネットワーク
上記のような背景から、1997年にHochreiterとSchmidhuberによって提案されたのが**長短期記憶（Long Short-Term Memory, LSTM）**ネットワークです ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=Long%20Short,term%20dependencies.))。LSTMはRNNの一種で、勾配消失を緩和し**長期の依存関係**を効率よく学習できるよう工夫されています ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=long,for%20standard%20RNNs%20to%20learn))。その鍵となるのが内部に導入された**セル状態**(cell state)と**ゲート機構**(gating mechanism)です ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=LSTMs%20overcome%20these%20challenges%20by,regulate%20the%20flow%20of%20information))。セル状態$c_t$は各時刻における長期的な情報を保持するためのもので、このセルに対して「必要な情報を流入・保持し、不必要な情報を削除し、出力に反映する」ことを**3つのゲート**が制御します ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=An%20LSTM%20unit%20is%20typically,0%20to%201%20to%20the))。3つのゲートとは以下の通りです ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=An%20LSTM%20unit%20is%20typically,0%20to%201%20to%20the)) ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=information%20from%20the%20cell%20state,))。

- **忘却ゲート (Forget Gate)** $f_t$：直前のセル状態$c_{t-1}$のうち、どの情報を**忘却（削除）するか**を決定します ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=gate%2C%20an%20output%20gate%2C,0%20to%201%20to%20the))。数式では  
  $$f_t = \sigma(W_f \mathbf{h}_{t-1} + U_f \mathbf{x}_t + \mathbf{b}_f),$$  
  というように、前時刻の隠れ状態$\mathbf{h}_{t-1}$と現在入力$\mathbf{x}_t$からシグモイド関数$\sigma$で$[0,1]$範囲の値を出力します ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=information%20from%20the%20cell%20state,))。この値が0に近ければ「その情報を完全に忘れる」、1に近ければ「保持する」ことを意味します ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=gate%2C%20an%20output%20gate%2C,0%20to%201%20to%20the))。

- **入力ゲート (Input Gate)** $i_t$：現在の入力からセル状態に**どの新情報を追加するか**を制御します ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=%28%5C%28%20f_t%20%5C%29%29,))。数式では  
  $$i_t = \sigma(W_i \mathbf{h}_{t-1} + U_i \mathbf{x}_t + \mathbf{b}_i),$$  
  と表され、忘却ゲートと同様に$0$～$1$の重みを決定します ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=%28%5C%28%20f_t%20%5C%29%29,))。また実際にセルに加える新情報の候補$\tilde{C}_t$も計算します：  
  $$\tilde{C}_t = \tanh(W_C \mathbf{h}_{t-1} + U_C \mathbf{x}_t + \mathbf{b}_C).$$  
  $\tanh$により$-1$～$1$の範囲の候補値を生成し、入力ゲート$i_t$によってそれがどの程度セル状態に反映されるかが決まります ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=%28%5C%28%20f_t%20%5C%29%29,))。

- **出力ゲート (Output Gate)** $o_t$：セルに記憶された情報のうち、どの部分を**出力（次の隠れ状態$\mathbf{h}_t$）に反映するか**を決めます ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=the%20previous%20state%2C%20by%20mapping,both%20in%20current%20and%20future))。数式では  
  $$o_t = \sigma(W_o \mathbf{h}_{t-1} + U_o \mathbf{x}_t + \mathbf{b}_o),$$  
  と計算されます ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=Creates%20new%20candidate%20values%20to,))。出力ゲートは現在のセル状態を参照して出力すべき情報を選択します。

以上のゲートの作用により、セル状態$c_t$と隠れ状態$h_t$は以下のように更新されます ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=o_t%20%3D%20%5Csigma%28W_o%20h_%7Bt,1))。

- **セル状態の更新**:  
  $$c_t = f_t \odot c_{t-1} + i_t \odot \tilde{C}_t,$$  
  すなわち前時刻のセル状態$c_{t-1}$は忘却ゲート$f_t$によって一部が削除され、代わりに現在の入力から得た新しい情報$\tilde{C}_t$が入力ゲート$i_t$によって追加されます ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=o_t%20%3D%20%5Csigma%28W_o%20h_%7Bt,1))（$\odot$は要素ごとの積）。

- **隠れ状態の更新**:  
  $$\mathbf{h}_t = o_t \odot \tanh(c_t),$$  
  更新されたセル状態$c_t$に$\tanh$を適用し、それに出力ゲート$o_t$を乗じたものが次の隠れ状態となります ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=cell%20state%20is%20updated%20by,1))。これがRNNとしての出力でもあり、必要に応じてさらに全結合層などを経て最終出力が得られます。

以上がLSTMセルの内部構造であり、図7.2に模式図を示します。LSTMの重要な点は、**勾配がセル状態経由でシーケンスを通じて流れやすい**設計になっていることです。例えば勾配消失の原因となる$tanh$や$\sigma$による縮小効果を、セル状態への恒等伝搬とゲートによる**スイッチ機構**で補っています。極端な場合、忘却ゲート$f_t \approx 1$かつ入力ゲート$i_t \approx 0$となれば$c_t \approx c_{t-1}$となり、勾配がそのまま伝播します（必要に応じて情報保持できるようになります）。このようにLSTMは**長期記憶**（Long-Term Memory）に相当するセル状態と、短期的な出力である隠れ状態$\mathbf{h}_t$を分離することで、「長い間忘れずにおいて後で必要になったら出力する」という動作を実現しています ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=Long%20short,since%20the%20early%2020th%20century)) ([Long short-term memory - Wikipedia](https://en.wikipedia.org/wiki/Long_short-term_memory#:~:text=An%20LSTM%20unit%20is%20typically,0%20to%201%20to%20the))。その結果、長い時系列データでも勾配消失の影響を受けにくくなり、RNNでは難しかった長期依存の学習に成功しました ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=Long%20Short,term%20dependencies.))。

 ([image]())図7.2: LSTMセルの構造 ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20score%28s_t%2Ch_i%29%20%3D%20v_%5Calpha,Another%20name%20for))。（上部緑のセルがセル状態の伝搬を示し、$\mathbf{x}_t$（下）と$\mathbf{h}_{t-1}$（左）の入力に対し、忘却ゲート$f_t$、入力ゲート$i_t$、出力ゲート$o_t$がそれぞれシグモイド層$\sigma$で計算される。これらゲートが前時刻からのセル状態$c_{t-1}$をどれだけ維持するか、現在の入力からどれだけ新情報を加えるか、そして出力隠れ状態$\mathbf{h}_t$にどの情報を反映するかを制御する ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=information%20from%20the%20cell%20state,)) ([Understanding LSTM Networks: Overcoming RNN Challenges - CliffsNotes](https://www.cliffsnotes.com/study-notes/19747604#:~:text=o_t%20%3D%20%5Csigma%28W_o%20h_%7Bt,1))。）

**GRU**（Gated Recurrent Unit）にも触れておきます。GRUはChoらによって提案されたゲート付きRNNで、LSTMからセル状態を除きゲートを2つ（リセットゲートと更新ゲート）に簡略化した構造です。GRUはパラメータが少ない分計算効率が良く、LSTMと同様に長期依存を捉えられることから広く用いられています。 ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20score%28s_t%2Ch_i%29%20%3D%20v_%5Calpha,Another%20name%20for))もっとも、本書の範囲では主にLSTMを代表として議論し、GRUの詳細説明は割愛します。

### 双方向RNN (Bidirectional RNN)
系列データによっては、**将来の文脈**が現在の出力決定に役立つ場合があります。双方向リカレントネットワーク（**Bi-RNN**）は、**前方向**（序列の先頭から末尾方向）へのRNNと**後方向**（末尾から先頭方向）へのRNNを組み合わせたアーキテクチャで、各時刻において過去と未来の両方の文脈を考慮した表現を得るものです ([Types of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#:~:text=Bi,The%20combined))。具体的には、ある入力系列$\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_T$に対し、通常の順方向の隠れ状態$\overrightarrow{\mathbf{h}}_t$に加え、逆順の隠れ状態$\overleftarrow{\mathbf{h}}_t$を計算し、それらを結合するなどして最終的な隠れ表現$\mathbf{h}_t = [\overrightarrow{\mathbf{h}}_t;\, \overleftarrow{\mathbf{h}}_t]$を得ます。この$\mathbf{h}_t$は入力系列の$t$周辺だけでなく**前後の文脈**を含む情報となります。双方向RNNは、品詞タグ付けや音声認識のように、系列全体が与えられた上で各位置の出力を求めるタスクで効果を発揮します ([Types of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#:~:text=Bi,The%20combined))。機械翻訳の文脈では、**エンコーダ（Encoder）**に双方向RNNを使いソース文全体の情報をエンコードすることで性能向上が図られました ([Types of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#:~:text=Bi,65))。特に双方向LSTMは単方向より性能が良いことが知られています ([Types of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#:~:text=Bi,65))。

> **補足:** 双方向RNNは将来の情報も参照できるとはいえ、「生成」タスクには直接は使えません。例えば翻訳の**デコーダ（Decoder）**は左から右へ順次ターゲット文を出力しますが、生成途中では将来の単語は未確定だからです。そのため通常、**エンコーダに双方向RNN**を用い、デコーダは単方向RNNで逐次生成します。第8章・第9章で解説するエンコーダ・デコーダ型モデルでもエンコーダ側に双方向LSTMが用いられています ([Types of artificial neural networks - Wikipedia](https://en.wikipedia.org/wiki/Types_of_artificial_neural_networks#:~:text=Bi,65))。

<br>

## 第8章：自然言語処理の基礎
本章では、自然言語処理（Natural Language Processing: **NLP**）におけるディープラーニングの基本技術として、テキストデータの前処理とベクトル化、およびエンコーダ・デコーダ型の**Sequence-to-Sequence (Seq2Seq)** モデルについて解説します。言語データは文字列（単語や文章）の形で与えられるため、そのままではニューラルネットワークの入力にできません。まずはテキストを扱う一般的な手順を確認し、その上でニューラルネットワークによる言語処理の基礎技術に踏み込んでいきます。

### テキスト前処理
**テキスト前処理**とは、生のテキストデータを解析・学習しやすい形式に整形する工程です ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=Natural%20Language%20Processing%20,text%20preprocessing%20for%20NLP%20tasks))。この段階では主に以下のような処理を行います。

- **クリーニング（不要物の除去）**：テキスト中のノイズとなる文字を除去・正規化します。例えばHTMLタグや特殊記号の削除、全角・半角の統一、数字や記号の一律置換などです。大文字小文字の統一（例：“Apple”→“apple”）なども一般的です ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=Why%20Text%20Preprocessing%20is%20Important%3F))。
- **トークン化 (Tokenization)**：テキストを単語やサブワード、文字などの**トークン**単位に分割します。例えば英語では空白と句読点で単語に区切る**単語分割**、日本語では形態素解析による単語への分割（わかち書き）などが該当します。トークン化によって、文字列がモデルに扱いやすい**離散的な単位列**に変換されます ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=Tokenization))。トークンは単語だけでなく場合によってはサブワード（BPEやSentencePieceなど）を用いることもあります。
- **ストップワード除去**：**ストップワード**とは頻出するが情報量の少ない単語（例：「の」「は」「and」「the」など）です。問題によってはこうした単語を分析から除外します ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=3))。
- **語幹化・原形化**：動詞や名詞の活用形を基本形に戻す処理です。例えば英語の`running`を`run`に変換する、`dogs`を`dog`にする、といった**ステミング**（語幹化）や**レンマ化**（原形への変換）が該当します ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=,Stop%20Words%20Removal))。日本語でも活用の統一を行うことがあります。
- **その他の正規化**：文脈に応じて、略語の展開（例： “don't” → “do not”）、絵文字や感情表現の変換、スペルミスの補正などを行います ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=,Spell%20Checking)) ([Text Preprocessing in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/text-preprocessing-for-nlp-tasks/#:~:text=Text%20Preprocessing%20Technique%20in%20NLP))。

以上の前処理により、テキストは例えば「"Apple is looking at buying U.K. startup for $1 billion"」という文が `["apple","look","buy","uk","startup","billion"]` といったトークン列に変換され、不要な情報が取り除かれます（この例では簡略化のため実際には様々な処理を一度に示しています）。なお、前処理の内容はタスクによって調整が必要です。例えば機械翻訳では原文の忠実な変換が求められるためストップワードを除かない場合が多いですが、情報検索ではストップワード除去が有用です。また近年の大規模言語モデルでは、サブワード分割（BPEなど）と最低限の正規化にとどめるケースも増えています。

最後に、テキストデータをモデルに入力するためには**語彙の構築**と**数値ベクトルへの変換**が必要です。一般的にはコーパス中に出現する全ての単語にIDを割り当てた**語彙辞書**を作り、各トークンをそのIDに置き換えます。例えば「apple→42, look→103, ...」のように整数ID列に変換します。これを**単語のインデックス化**と呼びます。未知語への対応として、訓練語彙になかった単語はすべて特殊トークン`<UNK>`に置き換える方法や、サブワード分割で未知語を減らす手法がとられます。

### 単語埋め込み (Word Embedding)
テキストのトークンを数値にしたとはいえ、**単語IDそのものには意味がありません**。例えば「apple=42」というIDはラベルにすぎず、数字の大小関係は単語の類似度とは無関係です。また単純にIDをone-hotベクトル（例えば語彙サイズ$N$の中で$42$番目だけが1で他は0のベクトル）にして神経網に入力すると、ベクトルが巨大かつ疎になる上、単語間の類似度を学習で捉えることが困難です ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embeddings%20are%20a%20method,will%20result%20in%20high%20computation))。そこで使用されるのが**単語埋め込み (Word Embedding)**と呼ばれる手法です。

**単語埋め込み**とは、**各単語を低次元の密なベクトル（密ベクトル）で表現する**ことです ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embeddings%20are%20numeric%20representations,applications%20in%20various%20NLP%20scenarios))。各単語に対応するベクトルを**単語ベクトル**もしくは**単語埋め込みベクトル**と呼びます。例えば語彙が1万語であっても、各単語を100次元程度の実数ベクトルで表すことが可能です。埋め込みベクトルは学習によって得られるもので、**単語の意味的・統計的な類似度を空間上の距離で表現する**ように訓練されます ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embeddings%20are%20numeric%20representations,applications%20in%20various%20NLP%20scenarios)) ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embedding%20is%20an%20approach,to%20have%20a%20similar%20representation))。その結果、「王（king）」と「女王（queen）」のベクトルが互いに近くに位置し、「王」と「林檎（apple）」のベクトルは遠く離れるといった、人間の直観に沿った幾何的関係が構築されます ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embedding%20is%20an%20approach,to%20have%20a%20similar%20representation))。

単語埋め込みを獲得する方法には、大きく分けて**学習タスクと同時に学習する方法**と**事前学習済みの埋め込みを利用する方法**があります。前者の場合、ニューラルネットワークモデルの一層目として**埋め込み層**(embedding layer)を設け、単語IDを対応するベクトルにマッピングします。この埋め込み行列は他の重みと同様に誤差逆伝搬で更新され、タスクに最適な表現に学習されます ([Word Embeddings in NLP - GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embeddings%20in%20NLP%20,capturing%20semantic%20and%20syntactic%20information))。後者の場合、例えば**Word2Vec**や**GloVe**といったアルゴリズムで大量の未ラベルコーパスから埋め込みをあらかじめ学習しておき、そのベクトル（例えば300次元）をモデルの初期埋め込みとして利用します ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embeddings%20are%20numeric%20representations,applications%20in%20various%20NLP%20scenarios)) ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=vector%20spaces,We))。有名な前者の手法としてはGoogleの**Word2Vec**（2013年）、Stanfordの**GloVe**（2014年）、Facebookの**FastText**（2016年）などがあり、これらはいずれも公開コーパスで学習済みの単語ベクトルを配布しています。

**Word2Vec**は特に自然言語処理で画期的だった手法で、単語の周辺文脈からその単語の埋め込みを学習します ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=vector%20spaces,We))。Word2Vecには**Continuous Bag-of-Words (CBOW)**モデルと**Skip-gram**モデルという2つのバリエーションがあります ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=There%20are%20two%20neural%20embedding,gram))。**CBOW**はある単語の前後$N$語を入力として、その単語自体を予測するタスクで埋め込みを学習します ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=match%20at%20L556%20used%20in,the%20center%20of%20the%20window))。一方**Skip-gram**は逆に、ある単語から周辺の単語を予測するタスクです。例えば“**king**”という単語を入力し、その文脈上周囲に現れるであろう単語（“crown”, “queen”, “royal”, ...など）を当てるように学習します。いずれの場合も、**「共起する単語はベクトル空間上近くなるように」**重みが調整されます ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=vector%20spaces,We)) ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=vectors,We))。訓練はニューラルネットワークで行われ、出現確率をソフトマックスで計算し交差エントロピー損失を最小化するといった枠組みです（詳細は割愛します）。Word2Vecによって得られたベクトルは、**ベクトル演算による意味的アナロジー**が可能なことでも知られています。「王 - 男 + 女 ≈ 女王（king - man + woman ≈ queen）」という有名な例は、対応するベクトルの差分がほぼ同一方向になることを示すものです ([The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-word2vec/#:~:text=The%20famous%20examples%20that%20show,%E2%80%9Cman%E2%80%9D%20%2B%20%E2%80%9Cwoman%E2%80%9D)) ([The Illustrated Word2vec – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-word2vec/#:~:text=Image%20The%20resulting%20vector%20from,we%20have%20in%20this%20collection))。

**GloVe**はWord2Vecとは異なり、コーパス中の**単語共起行列**の要素から埋め込みを学習する手法です。単語$i,j$の共起回数を$X_{ij}$とすると、GloVeでは2つの単語$i,j$のベクトル内積が$\log X_{ij}$を再現するように学習します。このように**統計的な共起情報を直接埋め込みに反映**させるアプローチであり、Word2Vecと並んで高品質なベクトルが得られることが知られています（論文ではWord2Vecの結果もかなり改善されています）。他にも、未知語への対処を組み込んだFastText（サブワード単位の埋め込み）など様々な埋め込み技術があります ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=match%20at%20L855%20Developed%20by,words%20and%20capturing%20morphological%20variations))。

まとめると、単語埋め込みにより**高次元で疎なone-hot表現**は**低次元で密な意味表現**に圧縮されます。埋め込み次元はタスクによりますが、50～300程度が一般的です。埋め込みベクトルには単語の意味・文法的な特徴が内包されており、機械学習モデルの性能を大きく向上させます ([Word Embeddings in NLP | GeeksforGeeks](https://www.geeksforgeeks.org/word-embeddings-in-nlp/#:~:text=Word%20Embeddings%20are%20numeric%20representations,applications%20in%20various%20NLP%20scenarios))。例えば**単語間の類似度**をコサイン距離などで測れば、「Paris」と「France」の類似度が「Paris」と「apple」より高い、といった結果が得られます。また埋め込み空間での**ベクトル演算**によって面白い関係性を発見できます（前述のking-man+woman=queenの例など）。図7.3はこうした単語埋め込みの概念を模式的に示しています。

> **実装Tip:** PyTorchでは`torch.nn.Embedding(num_embeddings, embedding_dim)`レイヤーを用いて簡単に単語埋め込みを利用できます。これは内部でサイズ`(num_embeddings, embedding_dim)`の重み行列を保持し、与えられた単語インデックスを対応する行ベクトルにマッピングします。訓練データが大量にない場合、`torchtext`や`gensim`を使って事前学習済みの埋め込み（例えばGloVeの6Bデータセットなど）を読み込み、Embeddingレイヤーに初期値としてセットすることも一般的です。

### Sequence-to-Sequenceモデルとエンコーダ・デコーダ
**Sequence-to-Sequence（Seq2Seq）モデル**は、長さの異なる入力系列から出力系列へと**可変長の変換**を行うためのニューラルネットワークアーキテクチャです ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=especially%20recurrent%20neural%20networks%20%28RNN%29,sequences%20of%20different%20lengths%2C%20making))。典型的には**エンコーダ・デコーダ（Encoder-Decoder）**方式が採用されます ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=are%20encoder,sequences%20of%20different%20lengths%2C%20making))。この方式では、ひとつのネットワーク（エンコーダ）が**入力系列**を固定長のベクトルに圧縮し、別のネットワーク（デコーダ）がそのベクトルから**出力系列**を生成します ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=are%20encoder,sequence%20given%20the%20input%20sequence))。2014年にSutskeverらが提案したこのモデルは、機械翻訳をはじめとする多くのNLPタスクで従来の手法を大きく上回る性能を示しました ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=Seq2Seq%20models%20addressed%20the%20issues,sequences%20of%20different%20lengths%2C%20making))。以下ではその基本原理を説明します。

**エンコーダ**はRNN（通常はLSTMやGRU）を用い、入力された系列$\mathbf{x}_1,\mathbf{x}_2,\dots,\mathbf{x}_T$を順次読み込んで隠れ状態を更新していきます ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=without%20attention%20are%20trained%20to,of%20the%20decoder%20hidden%20states))。最終的な入力の隠れ状態$\mathbf{h}_T$（あるいは全隠れ状態の集約）を、**文脈ベクトル**(context vector)と見なして固定長ベクトル$\mathbf{c}$として出力します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=without%20attention%20are%20trained%20to,of%20the%20decoder%20hidden%20states))。直感的には、エンコーダが**入力文の内容を圧縮して記憶**しているわけです。一方、**デコーダ**はこの文脈ベクトル$\mathbf{c}$から出発し、ターゲット系列$\mathbf{y}_1,\mathbf{y}_2,\dots$を一つずつ生成します ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=are%20encoder,sequence%20given%20the%20input%20sequence))。デコーダもRNNで実装され、まずデコーダの初期隠れ状態をエンコーダから渡された$\mathbf{c}$で初期化します（もしくは$\mathbf{c}$をデコーダへの入力に与えます） ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=without%20attention%20are%20trained%20to,of%20the%20decoder%20hidden%20states))。そして、デコーダはステップ毎に隠れ状態を更新しつつ出力単語を予測します。各ステップ$t$では、直前に予測・入力されたトークン$\mathbf{y}_{t-1}$と前時刻のデコーダ隠れ状態$\mathbf{s}_{t-1}$（エンコーダとは別にデコーダの隠れ状態を$s$で表すことが多い）から、新たな隠れ状態$\mathbf{s}_t$を計算し、そこから次の出力$\hat{\mathbf{y}}_t$の確率分布を得ます。この生成処理を、特殊トークン`<END>`が出力されるまで繰り返します。

以上がSeq2Seqモデルの動作の概要です。図8.1に典型的なエンコーダ・デコーダモデルの流れを示します。機械翻訳の場合であれば、エンコーダは**ソース言語文**をエンコードし、デコーダが**ターゲット言語文**を出力します。例えば「`X = [Bonjour, tout, le, monde]`（仏語）」を入力し、「`Y = [Hello, world, <END>]`（英語）」を出力する、といった具合です。

このモデルは**入力系列と出力系列の長さが異なっても対応できる**点が重要です ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=are%20encoder,sequence%20given%20the%20input%20sequence))。従来の多層パーセプトロンでは入力出力の次元が固定であるのに対し、Seq2SeqではRNNが系列を**可変長の内部状態**として処理するため、柔軟に対応できます。またエンコーダ・デコーダ構造により、入力と出力の言語やモダリティが異なっていても統一的に扱えます（画像キャプション生成では画像特徴量をエンコーダの出力$\mathbf{c}$として文章を生成する、など応用が可能）。

【※ 図8.1: エンコーダ・デコーダ型Seq2Seqモデルの概念図 （文章から別言語の文章への変換）】

しかし、この基本形のSeq2Seqモデルには一つ大きな課題がありました。それは**文脈ベクトル$\mathbf{c}$に情報を詰め込みすぎる**点です ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=hidden%20state%20from%20the%20encoder,sequence%20during%20the%20decoding%20step))。入力文が長く複雑になるほど、エンコーダ最終隠れ状態だけでそれを表現するのは困難になります。特に長文では、系列前半の情報がエンコーダを経由するうちに**忘れられてしまう**傾向がありました ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=hidden%20state%20from%20the%20encoder,sequence%20during%20the%20decoding%20step))。実際Sutskeverらの最初のモデルでも、入力を長くすると翻訳性能が極端に低下することが報告されています。この問題に対する解決策として提案されたのが**注意機構（Attention Mechanism）**で、次章で詳しく解説します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=bi,sequence%20during%20the%20decoding%20step)) ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=input%20sequences,sequence%20during%20the%20decoding%20step))。注意機構を導入したSeq2Seqでは、デコーダがエンコーダの出力全体から都度必要な情報を**検索**できるようになり、長文でも性能が飛躍的に向上しました。

> **学習について補足:**  エンコーダ・デコーダモデルの学習は教師あり学習で行われます。具体的には、コーパスの入力系列$X$と出力系列$Y$のペアに対し、モデルが$Y$を確率的に生成するよう**対数尤度（もしくは交差エントロピー損失）を最大化**します ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=hidden%20representation%20to%20generate%20output,sequence%20given%20the%20input%20sequence))。デコーダの訓練では、過去の出力としてモデルが予測した単語ではなく**正解の単語**$y_{t-1}$を入力する**教師強制 (teacher forcing)**が一般に用いられます。これにより効率的な学習が可能です。推論（デコード）時には、モデル自身の出力を次の入力として逐次処理（自回帰生成）します。また翻訳などでは、より尤度の高い系列を探索するために**ビームサーチ**などのデコード戦略が使われますが、本書の範囲では割愛します。

<br>

## 第9章：アテンションメカニズム
第8章で述べたように、注意（**Attention**）メカニズムはSeq2Seqモデルにおける画期的な改良手法です。2014年にBahdanauら【134】が機械翻訳モデルに導入し、続いてLuongら【136】が発展させました ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=proposed%20in%20Bahdanau%20et%20al,parts%20of%20the%20input%20sequence))。注意機構により、デコーダはエンコーダが出力した**すべての隠れ状態**を参照しながら翻訳を行えるようになり、従来の固定長ベクトルに情報を詰め込むボトルネックを解消しました ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=hidden%20state%20from%20the%20encoder,sequence%20during%20the%20decoding%20step)) ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=proposed%20in%20Bahdanau%20et%20al,parts%20of%20the%20input%20sequence))。本章では、この注意機構の仕組みと効果について詳しく説明します。

### 注意機構の考え方
直感的に言えば、Seq2Seqモデルのデコーダが出力を生成する際に**「エンコーダのどの部分に注目すべきか」**を学習で決めるのが注意機構です。具体的には、デコーダが時刻$t$の出力$y_t$を予測する際に、エンコーダが生成した一連の隠れ状態$\{\mathbf{h}_1,\mathbf{h}_2,\dots,\mathbf{h}_T\}$の中で、どの$\mathbf{h}_i$（入力のある位置$i$に対応）にどれだけ重みを置くかを計算します。この重みに基づいてエンコーダ側の情報の**重み付き平均**をとり、それをコンテキスト（文脈）情報$\mathbf{c}_t$として出力側に渡します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20h_i%20%3D%20%5B%5Coverrightarrow,T_x)) ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20hidden%20states%20%5C%28h_1%2C%5Cdots%2C%20h_))。こうすることで、デコーダは入力全体をまんべんなく見るのではなく、**その時に出力すべき内容に関連の深い入力部分**に注意を集中できるのです ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=proposed%20in%20Bahdanau%20et%20al,parts%20of%20the%20input%20sequence))。

注意機構の実装上は、デコーダの各ステップ$t$で以下の計算を行います ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=Second%2C%20an%20attention%20decoder%20does,the%20decoder%20does%20the%20following)) ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=1,trained%20jointly%20with%20the%20model))。

1. **スコア計算**：デコーダの前時刻の隠れ状態$\mathbf{s}_{t-1}$（多くの文献でデコーダ隠れ状態を$\mathbf{s}$で表します）と、エンコーダの各隠れ状態$\mathbf{h}_i$について、**関連度のスコア**$e_{t,i}$を計算します（後述する様々なスコア関数$score(\mathbf{s}_{t-1}, \mathbf{h}_i)$が提案されています） ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=1,hidden%20states%20with%20low%20scores)) ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=2,word%20of%20this%20time%20step))。
2. **重みの正規化**：スコア$e_{t,i}$にソフトマックス関数を適用し、各エンコーダ隠れ状態に対する**注意重み**$\alpha_{t,i}$を得ます ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20hidden%20states%20%5C%28h_1%2C%5Cdots%2C%20h_))。$\alpha_{t,i}$は0～1の実数で、全$i$について和が1となります。式で表すと、  
   $$\alpha_{t,i} = \frac{\exp(e_{t,i})}{\sum_{j=1}^{T} \exp(e_{t,j})},$$  
   に相当します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20%5Calpha_,layer%20feed))。これを**アライメント確率**とも呼びます（入力$i$の単語が出力$t$の単語に対応付けられる確率と解釈できます）。
3. **コンテキストベクトルの計算**：エンコーダの全隠れ状態の加重和をとり、**コンテキストベクトル**$\mathbf{c}_t$を計算します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20hidden%20states%20%5C%28h_1%2C%5Cdots%2C%20h_))。式では、  
   $$\mathbf{c}_t = \sum_{i=1}^{T} \alpha_{t,i}\, \mathbf{h}_i,$$  
   となります ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20hidden%20states%20%5C%28h_1%2C%5Cdots%2C%20h_))。重み$\alpha_{t,i}$が高い（＝注意を向けた）隠れ状態ほど強く反映された$\mathbf{c}_t$が得られます。
4. **デコーダ出力の計算**：最後に、コンテキスト$\mathbf{c}_t$とデコーダの現在の隠れ状態$\mathbf{s}_t$から、出力単語$y_t$の確率分布を計算します ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=3,for%20the%20next%20time%20steps))。Bahdanauらのモデルでは$\mathbf{c}_t$と$\mathbf{s}_t$を結合したベクトルに変換をかけて単語を予測しました ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=2,word%20of%20this%20time%20step))。Luongらの実装でも概ね同様ですが、コンテキスト$\mathbf{c}_t$を次時刻のデコーダへの入力にフィードバック（**入力フィーディング**）する工夫も提案しています。

以上がAttentionの基本的な流れです。肝となるのは**関連度スコア$e_{t,i}$の計算方法**で、いくつか種類があります ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20score%28s_t%2Ch_i%29%20%3D%20v_%5Calpha,1)) ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%28%5Ctext%7Bscore%7D%28%5Cboldsymbol%7Bs%7D_t%2C%20%5Cboldsymbol%7Bh%7D_i%29%20%3D%20%5Cboldsymbol%7Bs%7D_t%5E%5Ctop%5Cboldsymbol%7Bh%7D_i%5C%29%20Dot))。Bahdanauらが用いた手法は**アディティブ・アテンション（加法型）**とも呼ばれ、デコーダ隠れ状態$\mathbf{s}_{t-1}$とエンコーダ隠れ状態$\mathbf{h}_i$を結合したベクトルに1層のNNを適用してスコアを出します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=,is%20given%20as))。数式では次のようになります ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=architecture,is%20given%20as))。

$$
e_{t,i} = \mathbf{v}_a^T \tanh\!\Big(W_a [\,\mathbf{s}_{t-1};\, \mathbf{h}_i\,]\Big),
$$

ここで$\mathbf{v}_a, W_a$は学習されるパラメータ（重みベクトル・行列）で、$[\,\cdot;\cdot\,]$はベクトルの連結です。この手法は連結操作を行うためLuongらの分類では**Concat**とも呼ばれ、また後の文献では**Additive Attention**とも言われます ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20score%28s_t%2Ch_i%29%20%3D%20v_%5Calpha,Another%20name%20for))。一方Luongら（2015）は他にも以下のスコア関数を提案しました ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%5B%20score%28s_t%2Ch_i%29%20%3D%20v_%5Calpha,1)) ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=%5C%28%5Ctext%7Bscore%7D%28%5Cboldsymbol%7Bs%7D_t%2C%20%5Cboldsymbol%7Bh%7D_i%29%20%3D%20%5Cboldsymbol%7Bs%7D_t%5E%5Ctop%5Cboldsymbol%7Bh%7D_i%5C%29%20Dot))。

- **Dot**：$e_{t,i} = \mathbf{s}_{t-1}^T \mathbf{h}_i$（デコーダ隠れ状態とエンコーダ隠れ状態の内積）
- **General**：$e_{t,i} = \mathbf{s}_{t-1}^T W_a \mathbf{h}_i$（学習可能な行列$W_a$を介した内積）
- **Location-based**：$\displaystyle e_{t,i} = [W_a \mathbf{s}_{t-1}]_i$（デコーダ隠れ状態に基づく位置$i$固有のスコア。RNNではなくCNNエンコーダ向け）

一般的に、**Dot**は実装が簡単で計算量も少なく、ベースラインとしてよく使われます。**General**はDotの拡張で、異なる次元の場合やより柔軟なマッピングが必要な場合に有用です。**Concat (Additive)** はパラメータ数は増えますが表現力が高く、Bahdanauモデルで高い性能を示しました ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20weight%20matrices%20to%20be,mechanisms%20is%20the%20matrix%20of))。なおTransformer（注意は全てあなたに）では**Scaled Dot-Product Attention**（Dotの内積値をベクトル次元の平方根で割ったもの）が使われていますが、これは本質的にはDotタイプの注意です（スケーリングで勾配安定性を向上） ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20weight%20matrices%20to%20be,mechanisms%20is%20the%20matrix%20of))。

### 注意機構の効果と可視化
注意機構の導入により、Seq2Seqモデルは長大な系列に対しても性能が飛躍的に向上しました。特に**入力と出力で語順が異なる**ような翻訳タスクで顕著です。エンコーダは**双方向RNN**で文脈豊かな隠れ状態列を出力し、デコーダはそれらの中から適切な位置に注意を当てて翻訳していきます ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=proposed%20in%20Bahdanau%20et%20al,parts%20of%20the%20input%20sequence))。この振る舞いは**アライメント行列**として視覚化できます。

図9.1は、ある英仏翻訳モデルでデコーダが出力した単語とエンコーダの入力単語との対応を重みとして可視化したものです ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=Image%3A%20Alignment%20Matrix%20visualised%20for,Bahdanau%2C%20Cho%2C%20and%20Bengio%202014))。横軸が英語（入力）、縦軸がフランス語（出力）で、それぞれ単語が並んでいます。各セルの明るさが注意重み$\alpha_{t,i}$の大きさを表しており、明るい箇所ほど高い重み（強い対応）を示します ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=Image%3A%20Alignment%20Matrix%20visualised%20for,Bahdanau%2C%20Cho%2C%20and%20Bengio%202014))。

 ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/))図9.1: 注意機構による単語アライメントの例 ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=Image%3A%20Alignment%20Matrix%20visualised%20for,Bahdanau%2C%20Cho%2C%20and%20Bengio%202014))。英語文「The agreement on the European Economic Area was signed in August 1992 .」とフランス語文「L' accord sur la zone économique européenne a été signé en août 1992 .」との対応が示されている。白に近いマスほど対応する単語間の注意重みが高い（関連性が強い）ことを示す。

この図から、例えば「European Economic Area」という語句（英語）の翻訳「zone économique européenne」（フランス語）に着目すると、フランス語では語順が逆転しているにもかかわらず、モデルは適切に対応関係を学習していることがわかります ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=Image%20You%20can%20see%20how,sentence%20is%20in%20similar%20order))。実際、"European"（欧州の）に対して"européenne"（欧州の）へ、高い重みが割り当てられていますし ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=Image%20You%20can%20see%20how,sentence%20is%20in%20similar%20order))、"Area"（地域）に対して"zone"（地域）への重みが強くなっています。これは**Bahdanau注意**の論文【134】で示された例ですが、注意機構が単語の対応（アライメント）を内部で学習していることを示す代表的な可視化です。

このような**注意重みの可視化**はモデルの振る舞いの解釈に役立ちます。どの入力単語に注目して出力単語を生成したかが分かるため、Seq2Seqモデルがブラックボックスにならず、ある程度の説明性を持つことになります。加えて、Attention機構は応用も利きます。例えば画像キャプション生成ではCNNが抽出した画像の特徴マップ上の位置に対して注意重みを計算し、文章中のどの単語の生成に画像のどの領域が効いたかを可視化できます。Attentionは現在のディープラーニングにおいて幅広く使われる**汎用的な手法**となっています。

### 注意付きSeq2SeqとTransformerへの道
注意機構を組み込んだSeq2Seqモデル（**Attention Encoder-Decoder**）では、エンコーダとデコーダのやり取りが固定ベクトルではなく系列対系列のマッピングになります。デコーダの各ステップで異なる$\mathbf{c}_t$が入力されるため、長い文章でも情報損失が少なくなりました ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=proposed%20in%20Bahdanau%20et%20al,parts%20of%20the%20input%20sequence))。Luongらの論文では、Attentionなしでは翻訳品質が著しく低下する長文でも、Attentionありでは良好に翻訳できることが報告されています。

さらに2017年には、**Transformer**アーキテクチャが提案されました ([Chapter 8 Attention and Self-Attention for NLP | Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/attention-and-self-attention-for-nlp.html#:~:text=the%20weight%20matrices%20to%20be,mechanisms%20is%20the%20matrix%20of))。Transformerは極端なことに**RNN部分を完全に取り去り、Attention（自己注意）だけでエンコーダ・デコーダを構成したモデル**です。そのタイトル「Attention Is All You Need」が示す通り、シリーズの締めくくりとして本書の後半で詳細を扱いますが、背景にある基本原理はこの章で述べたものと同じです。Transformerでは**マルチヘッド・アテンション**という拡張が導入され、一度に複数種類の注意を並列計算することで表現力を上げています。いずれにせよ、**注意機構はディープラーニングによる系列モデリングを飛躍させた重要な発明**であり、RNN時代から現在のTransformer時代への橋渡しとなりました ([seq2seq Model in Machine Learning | GeeksforGeeks](https://www.geeksforgeeks.org/seq2seq-model-in-machine-learning/#:~:text=The%20advancement%20in%20neural%20networks,related%20tasks)) ([Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/#:~:text=I%20hope%20you%E2%80%99ve%20found%20this,Attention%20Is%20All%20You%20Need))。今後の章では、Attentionの応用であるTransformerについて学んでいきましょう。

