了解しました。第10章から第12章にかけての内容について、Transformerアーキテクチャ・事前学習言語モデル・最新動向と応用という三つのテーマを、それぞれ詳細かつ丁寧に、図や数式を交えて日本語でわかりやすくまとめます。画像は引用可能なものを活用し、Markdown形式で構成します。

準備が整いましたら、すぐにお知らせいたします。

## 第10章：Transformerアーキテクチャ

 ([KiKaBeN - Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/)) 図はTransformerのエンコーダ・デコーダ構造の概要です。左側がエンコーダ（Encoder）、右側がデコーダ（Decoder）を表しています ([KiKaBeN - Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/#:~:text=The%20transformer%20uses%20an%20encoder,translation))。Transformerはもともと機械翻訳のために提案されたモデルで、Encoder-Decoder方式を採用しています。**エンコーダ**では入力文（例えば英語の文章）を符号化して内部表現（ベクトル表現）に変換し、その出力を**デコーダ**に渡します ([KiKaBeN - Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/#:~:text=The%20transformer%20uses%20an%20encoder,translation))。**デコーダ**はエンコーダから受け取った内部表現をもとに、ターゲット言語（例えばフランス語）の文を逐次的に生成します。エンコーダとデコーダはいずれも複数の層（layers）を積み重ねて構成され、それぞれの層内で**Self-Attention**機構と**フィードフォワードネットワーク**（FFN）を含みます ([KiKaBeN - Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/#:~:text=The%20transformer%20uses%20an%20encoder,translation))。また、各層には**残差接続**（Residual Connection）と**Layer Normalization**（層正規化）が組み込まれており、これらによって学習を安定させ深いネットワークでも勾配が伝わりやすくなっています ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=%E5%89%8D%E7%AF%80%E3%81%AEAttention%E6%A9%9F%E6%A7%8B%E3%82%92%E7%A9%8D%E5%B1%A4%E3%81%97%E3%81%A4%E3%81%A4%E3%80%81%E7%B7%9A%E5%BD%A2%E5%B1%A4%E3%82%84%E6%AD%A3%E8%A6%8F%E5%8C%96%E5%B1%A4%E3%82%92%E9%81%A9%E5%88%87%E3%81%AB%E6%8C%9F%E3%81%BF%E8%BE%BC%E3%82%93%E3%81%A0%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E3%81%A8%E3%81%97%E3%81%A6Transformer%E3%81%AF%E6%8F%90%E6%A1%88%E3%81%95%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82%E3%81%BE%E3%81%9F%E3%80%81%E5%AF%BE%E8%B1%A1%E5%85%A5%E5%8A%9B%E3%82%92%E7%89%B9%E5%BE%B4%E3%83%99%E3%82%AF%20%E3%83%88%E3%83%AB%E3%81%A8%E3%81%97%E3%81%A6%E5%9F%8B%E3%82%81%E8%BE%BC%E3%82%80%E5%B1%A4%E3%82%84%E3%80%81%E4%BD%8D%E7%BD%AE%E6%83%85%E5%A0%B1%E3%82%92%E7%AC%A6%E5%8F%B7%E5%8C%96%E3%81%97%E3%81%A6%E4%BB%98%E5%8A%A0%E3%81%99%E3%82%8B%E5%B1%A4%E3%82%82%E9%87%8D%E8%A6%81%E3%81%AA%E6%A7%8B%E6%88%90%E8%A6%81%E7%B4%A0%E3%81%A7%E3%81%99%E3%80%82%E5%8E%9F%E5%85%B8%E3%81%AE%20Attention%20Is%20All,You%20Need%20%E3%81%A7%E3%81%AF%E7%BF%BB%E8%A8%B3%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%AB%E9%81%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%9F%E3%81%9F%E3%82%81%E3%80%81%E5%9F%8B%E3%82%81%E8%BE%BC%E3%81%BF%E5%B1%A4%E3%81%AF%E8%A8%80%E8%AA%9E%E3%81%AE%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E5%87%A6%E7%90%86%E3%81%A8%E3%81%97%E3%81%A6%E5%AE%9F%E8%A3%85%E3%81%95%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。入力単語はまず**埋め込み層**（Embedding Layer）でベクトルに変換され、さらに**位置エンコーディング**（Positional Encoding）のベクトルが加算されます ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=%E5%89%8D%E7%AF%80%E3%81%AEAttention%E6%A9%9F%E6%A7%8B%E3%82%92%E7%A9%8D%E5%B1%A4%E3%81%97%E3%81%A4%E3%81%A4%E3%80%81%E7%B7%9A%E5%BD%A2%E5%B1%A4%E3%82%84%E6%AD%A3%E8%A6%8F%E5%8C%96%E5%B1%A4%E3%82%92%E9%81%A9%E5%88%87%E3%81%AB%E6%8C%9F%E3%81%BF%E8%BE%BC%E3%82%93%E3%81%A0%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E3%81%A8%E3%81%97%E3%81%A6Transformer%E3%81%AF%E6%8F%90%E6%A1%88%E3%81%95%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82%E3%81%BE%E3%81%9F%E3%80%81%E5%AF%BE%E8%B1%A1%E5%85%A5%E5%8A%9B%E3%82%92%E7%89%B9%E5%BE%B4%E3%83%99%E3%82%AF%20%E3%83%88%E3%83%AB%E3%81%A8%E3%81%97%E3%81%A6%E5%9F%8B%E3%82%81%E8%BE%BC%E3%82%80%E5%B1%A4%E3%82%84%E3%80%81%E4%BD%8D%E7%BD%AE%E6%83%85%E5%A0%B1%E3%82%92%E7%AC%A6%E5%8F%B7%E5%8C%96%E3%81%97%E3%81%A6%E4%BB%98%E5%8A%A0%E3%81%99%E3%82%8B%E5%B1%A4%E3%82%82%E9%87%8D%E8%A6%81%E3%81%AA%E6%A7%8B%E6%88%90%E8%A6%81%E7%B4%A0%E3%81%A7%E3%81%99%E3%80%82%E5%8E%9F%E5%85%B8%E3%81%AE%20Attention%20Is%20All,You%20Need%20%E3%81%A7%E3%81%AF%E7%BF%BB%E8%A8%B3%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%AB%E9%81%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%9F%E3%81%9F%E3%82%81%E3%80%81%E5%9F%8B%E3%82%81%E8%BE%BC%E3%81%BF%E5%B1%A4%E3%81%AF%E8%A8%80%E8%AA%9E%E3%81%AE%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B%E5%87%A6%E7%90%86%E3%81%A8%E3%81%97%E3%81%A6%E5%AE%9F%E8%A3%85%E3%81%95%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。位置エンコーディングとは、系列中の単語の位置情報を符号化してモデルに与える仕組みであり、Transformerが単語の並び順を認識できるようにします（後述）。

**Self-Attention（自己注意）**はTransformerの核心となる仕組みです ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=%E6%9C%80%E3%82%82%E5%A4%A7%E4%BA%8B%E3%81%AA%E6%A7%8B%E6%88%90%E8%A6%81%E7%B4%A0%EF%BC%9AAttention))。Self-Attentionでは、系列中の各単語が他のすべての単語に対して**注意**（Attention）を払い、どの単語同士が関連性が高いかを学習します ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=Transformer%E3%81%AF%E3%80%81Attention%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E4%BB%95%E7%B5%84%E3%81%BF%E3%82%92%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E7%A9%8D%E5%B1%A4%E3%81%97%E3%81%9F%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82Attention%E3%82%92%E3%81%A8%E3%81%A6%E3%82%82%E6%8A%BD%E8%B1%A1%E7%9A%84%E3%81%AB%E8%A1%A8%E7%8F%BE%E3%81%99%E3%82%8B%E3%81%A8%E3%80%81%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E6%A4%9C%E7%B4%A2%E3%81%99%E3%82%8B%20%E3%81%9F%E3%82%81%E3%81%AE%E9%8D%B5%EF%BC%88Key%EF%BC%89%E3%81%A8%E5%AE%9F%E9%9A%9B%E3%81%AE%E5%80%A4%EF%BC%88Value%EF%BC%89%E3%81%AE%E3%83%9A%E3%82%A2%E9%9B%86%E5%90%88%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%80%81%E5%95%8F%E3%81%84%E5%90%88%E3%82%8F%E3%81%9B%EF%BC%88Query%EF%BC%89%E3%82%92%E6%8A%95%E3%81%92%E3%81%A6%E5%80%A4%E3%82%92%E5%8F%96%E3%82%8A%E5%87%BA%E3%81%99%E6%93%8D%E4%BD%9C%E3%81%A8%E3%81%84%E3%81%86%E8%AA%AC%E6%98%8E%E3%81%8C%E3%81%97%E3%81%A3%E3%81%8F%E3%82%8A%E3%81%8F%E3%82%8B%E3%81%AE%E3%81%A7%E3%81%AF%E3%81%AA%E3%81%84%E3%81%8B%E3%81%AA%E3%81%A8%E6%80%9D%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%20%E3%81%93%E3%81%AE%E3%81%A8%E3%81%8D%E3%80%81Query%E3%81%A8Key%E3%81%AF%E5%8E%B3%E5%AF%86%E3%81%AB%E4%B8%80%E8%87%B4%E3%81%99%E3%82%8B%E5%BF%85%E8%A6%81%E3%81%AF%E3%81%AA%E3%81%8F%E3%80%81%E5%90%84Query%E3%81%A8Key%E3%81%AE%E9%A1%9E%E4%BC%BC%E5%BA%A6%E3%81%AB%E5%9F%BA%E3%81%A5%E3%81%84%E3%81%A6%E9%80%A3%E7%B6%9A%E7%9A%84%E3%81%AB%E9%87%8D%E3%81%BF%E3%81%8C%E8%A8%88%E7%AE%97%E3%81%95%E3%82%8C%E3%80%81%E3%81%9D%E3%81%AE%E5%8A%A0%E9%87%8D%E5%B9%B3%E5%9D%87%E3%81%A8%E3%81%97%E3%81%A6Value%E3%81%8C%E5%BC%95%E3%81%8D%E5%87%BA%E3%81%95%E3%82%8C%E3%81%BE%20%E3%81%99%E3%80%82)) ([Transformerとは？数学を用いた徹底解説：Encoder編 #AI - Qiita](https://qiita.com/mantis522/items/b45494f4378b066d0432#:~:text=Self))。具体的には、各単語の埋め込みから**Query（問い合わせ）**、**Key（鍵）**、**Value（値）**という3種類のベクトルを線形変換で生成します ([Transformerとは？数学を用いた徹底解説：Encoder編 #AI - Qiita](https://qiita.com/mantis522/items/b45494f4378b066d0432#:~:text=Self))。例えば単語集合を表す行列$X$に対し、重み行列$W^Q, W^K, W^V$でそれぞれ変換を行い、$Q=XW^Q, K=XW^K, V=XW^V$を得ます。AttentionではまずQueryと全てのKeyとの類似度（内積）を計算し、その結果にsoftmax関数を適用して重みを求めます。その重みを各Valueに乗じて総和を取ることでAttentionの出力が得られます。これを数式で表すと、**\[Attention(Q,K,V) = \text{softmax}\Big(\frac{QK^T}{\sqrt{d_k}}\Big)V\]**です ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=%7B%5Crm%20Attention%7D%28Q%2C%20K%2C%20V%29%3D%7B%5Crm%20softmax%7D%5Cleft%28%5Cfrac%7BQK,d_v))。ここで$d_k$はベクトルの次元で、内積のスケールを調整するために平方根$\sqrt{d_k}$で割り、softmax関数により重み付け和の計算を行っています ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=%7B%5Crm%20Attention%7D%28Q%2C%20K%2C%20V%29%3D%7B%5Crm%20softmax%7D%5Cleft%28%5Cfrac%7BQK,d_v))。softmaxにより各単語への注意の合計が1となるよう正規化され、類似度の高い単語ほど大きな重みが与えられます。このようにして、Self-Attentionはある単語と他の単語との関係性を動的に考慮した新たな表現を計算します。例えば、英語の文「The animal didn't cross the street because **it** was too tired.」において、`it`という単語をエンコードする際、Self-Attentionは他の単語（`animal`など）との関連性を考慮し、`it`が指すものを文脈から把握できるようにします ([The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time.](https://jalammar.github.io/illustrated-transformer/#:~:text=match%20at%20L172%20Image%20As,it))。

**マルチヘッドAttention（Multi-Head Attention）**は、上記のAttention機構を並列に複数設ける拡張です。単一のAttentionでは1種類の関連性しか学習できませんが、マルチヘッドにすることで**異なる視点**・**サブスペース**でAttentionを計算できます ([KiKaBeN - Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/#:~:text=and%2For%20function.%20Hence%2C%20the%20self,please%20refer%20to%20this%20article))。具体的には、重み行列$W^Q, W^K, W^V$にヘッドごとの独立なセットを用意し、それぞれから得られるAttention出力（ヘッド）を結合して用います。数式で表すと、各ヘッド$head_i$を$head_i = Attention(QW_i^Q,\;KW_i^K,\;VW_i^V)$と計算し、それらを連結した後に最終の重み$W^O$を乗じて出力を得ます ([Attention (machine learning) - Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning)#:~:text=Multi,O%7D%7D%20are%20parameter))。つまり **\[MultiHead(Q,K,V) = \text{Concat}(head_1,\dots, head_h)W^O\]** であり、$W_i^Q, W_i^K, W_i^V, W^O$は学習パラメータです ([Attention (machine learning) - Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning)#:~:text=Multi,O%7D%7D%20are%20parameter))。マルチヘッドにより、Attention機構は**複数の特徴空間**にわたって情報を抽出でき、文法的な関係を見るヘッド、意味的な関係を見るヘッド、といった具合に多様な関連性を同時に学習できます ([KiKaBeN - Transformer’s Encoder-Decoder](https://kikaben.com/transformers-encoder-decoder/#:~:text=and%2For%20function.%20Hence%2C%20the%20self,please%20refer%20to%20this%20article))。また計算を並列化できるため効率も損なわれません ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=match%20at%20L515%20This%20led,33))。

**位置エンコーディング（Positional Encoding）**は、系列中の単語の順序情報をモデルに与えるための工夫です。TransformerにはRNNのような順番に逐次処理する構造がないため、単語の**位置**を別途埋め込む必要があります。原論文では**正弦波（サイン波）**に基づく位置エンコーディングが提案されました ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=original%20paper,2%2Fd))。例えば位置$t$に対するエンコーディングを、偶数次元では$\sin$、奇数次元では$\cos$関数で生成します ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=original%20paper,2%2Fd))。具体的な定義としては、次元$2k$には$\sin(t/10000^{2k/d_{\text{model}}})$、次元$2k+1$には$\cos(t/10000^{2k/d_{\text{model}}})$の値を用います ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=original%20paper,2%2Fd))。このようにして各位置に一意な振動パターンを割り当て、単語の相対的位置関係をモデルが学習できるようにしています。位置エンコーディングは学習不要な固定の方法ですが、十分な表現能力を持ち、位置が離れた単語同士の関係も捉えられるようになっています。また近年ではRoPE（Rotation Positional Encoding）など他の手法も提案されていますが ([Transformer (deep learning architecture) - Wikipedia](https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)#:~:text=,40))、いずれも**系列長に依存しない位置表現**を与える点で共通しています。

**Layer Normalization（層正規化）**と**Residual Connection（残差接続）**は、Transformerの各サブレイヤー（Self-AttentionやFFN）における安定化のために用いられます。残差接続とは、サブレイヤーの入力をその出力に足し合わせる仕組みで、ResNetに由来します。これにより勾配消失を防ぎ、ネットワークが深くても学習しやすくなります。Layer Normalizationは各層の出力を平均0・分散1に正規化する操作で、内部共変量シフトを抑制し学習の安定化に寄与します。Transformerでは各残差接続の後にLayerNormを適用する「**Post-LN**」構造が採用されており、元の入力とサブレイヤー出力を足し合わせた後に正規化します ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=match%20at%20L341%20Fig.2%20%E3%81%8B%E3%82%89%E3%82%82%E3%82%8F%E3%81%8B%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%80%81%E9%80%9A%E5%B8%B8%E3%81%AETransformer%E3%81%A7%E3%81%AF%E6%AD%A3%E8%A6%8F%E5%8C%96%E5%B1%A4%EF%BC%88LayerNorm%EF%BC%89%E3%81%AF%E5%90%84%E6%AE%8B%E5%B7%AE%E6%8E%A5%E7%B6%9A%E3%81%AE%E5%BE%8C%E3%81%AB%E9%85%8D%E7%BD%AE%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E3%81%AE%E3%81%A1%E3%81%AB%E3%81%93%E3%81%AE%E3%82%BF%E3%82%A4%E3%83%97%E3%81%AFPost,LN%E3%81%AF%E5%B1%A4%E3%81%8C%E6%B7%B1%E3%81%8F%E3%81%AA%E3%82%8B%E3%81%BB%E3%81%A9%E5%AD%A6%E7%BF%92%E3%81%8C%E6%AF%94%E8%BC%83%E7%9A%84%E4%B8%8D%E5%AE%89%E5%AE%9A%E3%81%A8%E3%81%AA%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E7%9F%A5%E3%82%89%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82))（図の「Add & Norm」がそれです）。一方、後の研究ではLayerNormをサブレイヤーの前に置く「Pre-LN」も提案され、より安定に学習できる場合もあると報告されています ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=Fig.2%20%E3%81%8B%E3%82%89%E3%82%82%E3%82%8F%E3%81%8B%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%80%81%E9%80%9A%E5%B8%B8%E3%81%AETransformer%E3%81%A7%E3%81%AF%E6%AD%A3%E8%A6%8F%E5%8C%96%E5%B1%A4%EF%BC%88LayerNorm%EF%BC%89%E3%81%AF%E5%90%84%E6%AE%8B%E5%B7%AE%E6%8E%A5%E7%B6%9A%E3%81%AE%E5%BE%8C%E3%81%AB%E9%85%8D%E7%BD%AE%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E3%81%AE%E3%81%A1%E3%81%AB%E3%81%93%E3%81%AE%E3%82%BF%E3%82%A4%E3%83%97%E3%81%AFPost))。いずれにせよ、正規化層は勾配のスケールを整えることで学習を円滑にし、残差接続は情報伝達をスムーズにすることで深いモデルを効果的に機能させています。

まとめると、Transformerアーキテクチャは**Attention機構を中核**に据えつつ、並列計算や長距離依存関係の学習を可能にした画期的なモデルです。その全体構造はEncoder-Decoderからなり、Self-AttentionとMulti-Head Attentionによって入力系列中のあらゆる依存関係を効果的に捉えます。さらに残差接続とLayerNormにより学習を安定化させ、位置エンコーディングによって系列順序を取り込みます。2017年の登場以来、このTransformerアーキテクチャは機械翻訳のみならずさまざまなNLPタスクで従来のRNNに代わる基盤技術となり、後述するBERTやGPTといった**事前学習言語モデル**の登場へと繋がっていきます ([Transformerとは？数学を用いた徹底解説：Encoder編 #AI - Qiita](https://qiita.com/mantis522/items/b45494f4378b066d0432#:~:text=RNN%E3%81%A8LSTM%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AF%E3%80%81%E6%AC%A1%E3%81%AE%E5%8D%98%E8%AA%9E%E4%BA%88%E6%B8%AC%E3%80%81%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3%E3%80%81%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E7%94%9F%E6%88%90%E3%81%AA%E3%81%A9%E3%81%AE%E9%A0%86%E6%AC%A1%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%A7%E5%BA%83%E3%81%8F%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%E3%81%97%E3%81%8B%E3%81%97%E3%80%81%E3%81%93%E3%81%AE%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AB%E3%81%AF%E9%95%B7%E6%9C%9F%E4%BE%9D%E5%AD%98%E6%80%A7%E3%81%AE%E5%95%8F%E9%A1%8C%E3%81%8C%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82%20%E3%81%93%E3%81%AERNN%E3%81%AE%E9%99%90%E7%95%8C%E7%82%B9%E3%82%92%E5%85%8B%E6%9C%8D%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AB%E3%80%81%E3%80%8CAttention%20is%20all%20you,3%2C%20T5%E3%81%AA%E3%81%A9%E3%81%AE%E9%9D%A9%E5%91%BD%E7%9A%84%E3%81%AA%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E3%81%8C%E7%99%BA%E5%B1%95%E3%81%99%E3%82%8B%E5%9F%BA%E7%9B%A4%E3%81%8C%E4%BD%9C%E3%82%89%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82)) ([Transformerとは？数学を用いた徹底解説：Encoder編 #AI - Qiita](https://qiita.com/mantis522/items/b45494f4378b066d0432#:~:text=Transformer%E3%81%AFRNN%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%9F%E5%BE%AA%E7%92%B0%E6%96%B9%E5%BC%8F%E3%82%92%E4%BD%BF%E3%82%8F%E3%81%9A%E3%80%81%E7%B4%94%E7%B2%8B%E3%81%ABAttention%E3%81%A0%E3%81%91%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82Transformer%E3%81%AFself))。

## 第11章：事前学習言語モデル

事前学習言語モデル（Pre-trained Language Model）は、大規模な未分類テキストで**自己教師あり学習**を行い、言語の汎用的な理解や生成能力を獲得したモデルです。その代表例として**BERT**（Bidirectional Encoder Representations from Transformers）と**GPT**（Generative Pre-trained Transformer）があります。これらはTransformerアーキテクチャを基盤としつつ、それぞれ異なる事前学習タスクを設定することで、言語表現を効果的に学習しています ([Transformerとは？数学を用いた徹底解説：Encoder編 #AI - Qiita](https://qiita.com/mantis522/items/b45494f4378b066d0432#:~:text=RNN%E3%81%A8LSTM%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AF%E3%80%81%E6%AC%A1%E3%81%AE%E5%8D%98%E8%AA%9E%E4%BA%88%E6%B8%AC%E3%80%81%E6%A9%9F%E6%A2%B0%E7%BF%BB%E8%A8%B3%E3%80%81%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E7%94%9F%E6%88%90%E3%81%AA%E3%81%A9%E3%81%AE%E9%A0%86%E6%AC%A1%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%A7%E5%BA%83%E3%81%8F%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%E3%81%97%E3%81%8B%E3%81%97%E3%80%81%E3%81%93%E3%81%AE%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AB%E3%81%AF%E9%95%B7%E6%9C%9F%E4%BE%9D%E5%AD%98%E6%80%A7%E3%81%AE%E5%95%8F%E9%A1%8C%E3%81%8C%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82%20%E3%81%93%E3%81%AERNN%E3%81%AE%E9%99%90%E7%95%8C%E7%82%B9%E3%82%92%E5%85%8B%E6%9C%8D%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AB%E3%80%81%E3%80%8CAttention%20is%20all%20you,3%2C%20T5%E3%81%AA%E3%81%A9%E3%81%AE%E9%9D%A9%E5%91%BD%E7%9A%84%E3%81%AA%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A3%E3%81%8C%E7%99%BA%E5%B1%95%E3%81%99%E3%82%8B%E5%9F%BA%E7%9B%A4%E3%81%8C%E4%BD%9C%E3%82%89%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82)) ([Transformerとは？数学を用いた徹底解説：Encoder編 #AI - Qiita](https://qiita.com/mantis522/items/b45494f4378b066d0432#:~:text=Transformer%E3%81%AFRNN%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%97%E3%81%9F%E5%BE%AA%E7%92%B0%E6%96%B9%E5%BC%8F%E3%82%92%E4%BD%BF%E3%82%8F%E3%81%9A%E3%80%81%E7%B4%94%E7%B2%8B%E3%81%ABAttention%E3%81%A0%E3%81%91%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%9F%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82Transformer%E3%81%AFself))。事前学習したモデルは、その後**転移学習（Transfer Learning）**によって下流の特定タスクに**ファインチューニング**（fine-tuning）されます ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=limits%20its%20effectiveness%20in%20understanding,specific%20datasets)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=%2A%20Non,robust%20initial%20understanding%20of%20language))。これにより、少量のタスク固有データしかない場合でも高性能なモデルを構築できるのが大きな利点です。

**BERT**はエンコーダのみから成るTransformerで、**双方向**の文脈理解に特化したモデルです ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=BERT%2C%20developed%20by%20researchers%20at,are%20the%20hallmarks%20of%20BERT)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=%2A%20Non,tasks%2C%20though%20it%20generally%20requires))。BERTの事前学習では**マスク言語モデル（Masked Language Modeling, MLM）**と**次文予測（Next Sentence Prediction, NSP）**という自己教師ありタスクが用いられました ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=precede%20them%29,hallmarks%20of%20BERT))。MLMでは入力文中の一部単語を特殊トークン`[MASK]`に置換し、モデルにその隠れた単語を当てさせます ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=precede%20them%29,hallmarks%20of%20BERT))。例えば「今日は[MASK]へ行く」という入力に対し、BERTは文脈から[MASK]が「学校」なのか「会社」なのかを予測します。モデルは単語左右の両文脈を同時に参照できるため（双方向性）、文中のあらゆる情報を統合して欠損単語を推測することを学習します ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=precede%20them%29,hallmarks%20of%20BERT))。この双方向の学習により、BERTは言葉の意味やニュアンスを深く捉えた表現を内部に獲得します。一方、NSPではある文の直後に特定の文が続くかどうかを当てるタスクで、文と文の関係性（連続性や一貫性）を学習させます。BERTはMLMとNSPで大量のテキストから学習することで、文脈理解に優れた**言語エンコーダ**としての能力を手に入れました ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=,tasks%2C%20though%20it%20generally%20requires))。事前学習後、BERTは各種NLPタスク（質問応答、文書分類、命名体認識など）のデータでファインチューニングされますが、その際は出力層だけをタスクに合わせて付け替え、多くの場合数エポックの学習で高い性能が発揮できます ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=%2A%20Non,robust%20initial%20understanding%20of%20language))。これは事前学習で既に言語の深い理解を持っているためで、BERTでは下流タスクごとの学習データが少なくても高精度を達成できることが知られています ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=%2A%20Non,initial%20understanding%20of%20language%20context))。

 ([BERT Architecture Explained for Beginners](https://www.analyticsvidhya.com/blog/2022/11/comprehensive-guide-to-bert/)) 一方、**GPT**はデコーダのみから成るTransformerで、**一方向**（順方向）の文脈に基づく**自己回帰モデル（Auto-Regressive Model）**です ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=GPT%2C%20developed%20by%20OpenAI%2C%20is,on%20the%20input%20it%20receives)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=,full%20context%20of%20the%20input))。GPTの事前学習タスクは非常にシンプルで、与えられたテキストの続きを予測するというものです。すなわち「言語モデルによる次単語予測」のタスクで、大量の文章を読み込みながら常に次に来る単語を当てるよう学習します ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=GPT%2C%20developed%20by%20OpenAI%2C%20is,on%20the%20input%20it%20receives)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=,full%20context%20of%20the%20input))。例えば「今日は学校へ行く途中で犬に」の後に来る単語を予測するといった具合です。GPTはこれを左から右への一方向で行うため、ある単語を生成する時点では**それ以降の単語は見えません**。このような**順次的生成**の訓練により、GPTは文章の続きを自然に作り出す**言語生成モデル**としての能力を獲得します ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=GPT%2C%20developed%20by%20OpenAI%2C%20is,on%20the%20input%20it%20receives)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=,full%20context%20of%20the%20input))。GPTが優れている点は、大規模テキストコーパスから文法的に正しく、かつ文脈に整合したテキストを生成できるようになることです。事前学習後、GPTも下流タスク（文章要約や機械翻訳、対話生成など）にファインチューニングされますが、**生成タスク**において強みを発揮します ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=GPT%2C%20developed%20by%20OpenAI%2C%20is,on%20the%20input%20it%20receives)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=%2A%20Non,robust%20initial%20understanding%20of%20language))。例えばGPT-2やGPT-3は極めて流暢な文章生成が可能であり、与えられたプロンプト（入力文脈）に続く自然なテキストを産み出します。その一方で、BERTのように双方向文脈を完全には考慮できないため、長い入力の理解や精密な分析にはやや不向きとされます。ただし最新のGPTシリーズでは自己注意のマスクを工夫することで、ある程度双方向的な文脈利用も取り入れています。

BERTとGPTの違いをまとめると、**BERTはエンコーダ型（双方向）で主に理解タスク向き、GPTはデコーダ型（単方向）で生成タスク向き**という点にあります ([BERT vs GPT: Comparing the Two Most Popular Language Models](https://blog.invgate.com/gpt-3-vs-bert#:~:text=Models%20blog,model%2C%20while%20BERT%20is)) ([What is the difference between GPT blocks and BERT blocks](https://datascience.stackexchange.com/questions/87637/what-is-the-difference-between-gpt-blocks-and-bert-blocks#:~:text=What%20is%20the%20difference%20between,only))。BERTはMasked LMによって前後文脈を同時に考慮した深い言語理解を得ており、文章分類や質問応答などで高性能を示します。一方GPTはAuto-Regressiveな訓練によって次単語予測に特化した生成能力を持ち、物語の自動生成や対話システムなどで威力を発揮します ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=GPT%2C%20developed%20by%20OpenAI%2C%20is,on%20the%20input%20it%20receives)) ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=,tasks%2C%20though%20it%20generally%20requires))。また**モデル構造**にも違いがあります。BERTは12層（BASEの場合）または24層（LARGE）のTransformerエンコーダからなり、各層にSelf-AttentionとFFNが含まれます。一方GPT-2（117Mパラメータ版）は12層のTransformerデコーダから成り、後方（未来）の単語へのAttentionをマスクした形でSelf-Attentionを行います（将来の情報が漏れないようにするため） ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=%E7%BF%BB%E8%A8%B3%E6%96%87%E3%81%AE%E7%94%9F%E6%88%90%E3%82%92%E6%8B%85%E5%BD%93%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AF%E3%83%87%E3%82%B3%E3%83%BC%E3%83%80%E3%81%A7%E3%81%99%EF%BC%88Fig.2%20%E5%8F%B3%E5%81%B4%EF%BC%89%E3%80%82%E3%81%93%E3%81%93%E3%81%A7%E3%81%AF%E3%80%81%E6%9C%80%E5%88%9D%E3%81%AB%20%60,EOS%5D%60%EF%BC%88End%20Of%20Sentence%EF%BC%9A%E6%96%87%E6%9C%AB%EF%BC%89%E3%81%A8%E3%81%84%E3%81%86%E7%89%B9%E6%AE%8A%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%97%E3%81%9F%E3%82%89%E7%94%9F%E6%88%90%E7%B5%82%E4%BA%86%E3%81%A7%E3%81%99%E3%80%82))。両者とも事前学習には膨大な計算資源とテキストデータを用いており（BERTはBooksCorpusやWikipedia、GPT-3はWebから集めた数千億トークン）、この大規模事前学習こそがそれぞれの高性能の源泉です。

事前学習言語モデルの**ファインチューニング**では、一般にモデル全体を下流タスク用データでさらに訓練しますが、事前学習で得た知識が土台としてあるため少ないデータでも高い効果が出ます ([Differences Between GPT and BERT | GeeksforGeeks](https://www.geeksforgeeks.org/differences-between-gpt-and-bert/#:~:text=%2A%20Non,initial%20understanding%20of%20language%20context))。例えばBERTではわずか数エポックの追加学習で多くのNLPベンチマークで最先端性能を達成しました。また場合によっては、モデルの大部分を凍結しごく一部のパラメータだけを訓練するアプローチ（例えばAdapterチューニングやLoRAといった手法）も取られます。これにより低リソース環境でも巨大モデルを効率良く適用できるようになります。実際、少数の追加パラメータを学習するだけで新しいタスクに適応できることが報告されています ([Pre-training Vs. Fine-Tuning Large Language Models](https://www.ankursnewsletter.com/p/pre-training-vs-fine-tuning-large#:~:text=Pre,to%20specific%20tasks%20or))。

**モデルの解釈性**と**限界**についても触れておきます。Transformerベースの言語モデルはその内部表現が膨大な次元に渡るため**ブラックボックス**になりがちで、人間が直接理解するのは容易ではありません。しかし、BERTの場合は**Attentionの重み**を可視化することで「どの単語がどの単語に注意を払っているか」を分析する試みがなされました ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=match%20at%20L63%20Image%20Fig,Matr%20ix%E3%82%92%E8%A1%A8%E3%81%97%E3%81%A6%E3%81%8A%E3%82%8A%E3%80%81%E3%83%88%E3%83%BC%E3%82%AF%E3%83%B3%E5%90%8C%E5%A3%AB%E3%81%8C%E5%BC%B7%E3%81%8F%E5%8F%8D%E5%BF%9C%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E7%AE%87%E6%89%80%E3%81%8C%E6%BF%83%E3%81%84%E8%89%B2%E3%81%A7%E8%A1%A8%E7%A4%BA%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%80%82%EF%BC%88%E4%B8%AD%E5%A4%AE%E5%9B%B3%E3%81%AE%E8%A3%9C%E8%B6%B3%EF%BC%9AFrameNet%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E8%AA%9E%E5%BD%99%E5%90%8C%E5%A3%AB%E3%81%AE%E6%8E%A5%E7%B6%9A%E3%81%AB%E9%96%A2%E3%81%99%E3%82%8B%E3%83%87%E3%83%BC%E3%82%BF%E3%82%BB%E3%83%83%E3%83%88%E3%81%AB%E5%9F%BA%E3%81%A5))。これはモデルが文章内で何を重要だと判断しているかの手掛かりになります。ただし近年の研究では、Attention重みだけではモデルの判断根拠を必ずしも正確に示せない場合も指摘されています。それでもBERTやGPTから得られる単語ベクトル（埋め込み）は意味的な類似度を持つことが知られ、例えばベクトル空間上で類義語が近傍に配置されるなど、モデル内部に言語知識が暗黙的にエンコードされていることが分かっています。

限界としては、**事前学習コーパスに存在しない知識は答えられない**点や、特にGPT系モデルでは**事実に基づかない内容（幻覚）を流暢に生成してしまう**問題があります。また、BERTは生成タスクに不向きであるように、モデルの特性によって得意不得意があることも限界の一つです。さらに後述するように、大規模言語モデルには**社会的バイアス**や**有害な表現**が含まれてしまうリスクもあります。これらの課題に対して、近年はファインチューニング時に人間のフィードバックを取り入れて出力を調整する手法（例えばInstructGPTやChatGPTにおける**RLHF**: 人間フィードバックによる強化学習）が採用されています。また、生成結果にフィルタを掛ける、知識グラフやツールと組み合わせて事実性を向上させる、といった工夫もなされています。モデルの解釈性についても、Attention以外に**勾配入力解析**や**モデル可視化**の技法が模索されていますが、依然Transformerの判断過程を完全に説明するのは難しく、これが今後の研究課題です。

## 第12章：Transformerの最新動向と応用

2018年以降、Transformerを基盤とする**巨大言語モデル（LLM: Large Language Model）**が飛躍的な発展を遂げました。モデルのパラメータ数は数億から数千億、果ては数兆に達し、膨大なテキストコーパスで事前学習されたLLMは驚異的な言語生成・理解能力を示しています。例えばOpenAIのGPT-3（2020年、1750億パラメータ）や、GoogleのPaLM（2022年、5400億パラメータ）などが知られます。これらLLMでは**スケーリング則**と呼ばれる経験則があり、モデル規模・データ量・計算量を増やせば増やすほど性能が向上することが報告されています ([30分で完全理解するTransformerの世界](https://zenn.dev/zenkigen_tech/articles/2023-01-shimizu#:~:text=Transformer%E3%81%AE%E3%82%B9%E3%82%B1%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E5%89%87))。実際、GPT-3の論文ではモデルサイズを段階的に増やした際の性能向上が示され、パラメータを増やすことで新たな能力（ゼロショット推論や常識推論など）が「**出現**」することが確認されました。また、これらLLMは**Few-Shot Learning**（少数例だけで新タスクに適応）や**In-Context Learning**（追加の勾配更新なしでプロンプト内の例示によってタスクをこなす）といった特性を示し、従来のモデルにはない柔軟性を持っています。

しかし、モデル巨大化に伴う計算資源・メモリの問題から、Transformerの**効率化手法**も数多く研究されています。標準的な自己注意の計算量はシーケンス長$n$に対して$O(n^2)$と二乗オーダーであり、長大な入力に対処するには負荷が大きいです。これを改善するために、いくつかのアプローチが提案されています。**Performer**はランダム特徴写像（Random Feature）によってソフトマックスAttentionを近似し、Attention計算を線形時間に削減する手法です ([Brief Review — Rethinking Attention with Performers | by Sik-Ho Tsang | Medium](https://sh-tsang.medium.com/brief-review-rethinking-attention-with-performers-e9fba834ab95#:~:text=,very%20long%20protein%20sequence))。Performerでは$O(n^2)$の注意行列を