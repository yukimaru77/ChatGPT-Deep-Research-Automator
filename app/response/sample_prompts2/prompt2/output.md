了解しました。第4章から第6章に関して、情報系大学院1年生向けに丁寧で専門的な日本語解説資料を作成します。画像付きで、複数の高品質な資料を統合し、内容をごまかさず詳細に説明します。

章ごとの構成に従って、ニューラルネットワークの基礎、深層学習フレームワーク（PyTorch中心）、畳み込みニューラルネットワークについて、図解・数式・コードを含む完全なマークダウン形式の教材にまとめてお届けします。完了次第ご連絡いたします。

## 第4章：ニューラルネットワークの基礎

### 生物学的ニューロンとパーセプトロン
生物の脳内にある**ニューロン（神経細胞）**は、樹状突起で他のニューロンからの刺激を受け取り、細胞体で信号を統合し、一定の閾値を超えると軸索を通じて他のニューロンへ電気信号を送ります ([人工ニューロンとパーセプトロン〖機械学習再勉強〗コード追記 #Python - Qiita](https://qiita.com/YutamaKotaro/items/d7908b24b5cf60f5c92c#:~:text=%E7%94%9F%E7%89%A9%E5%AD%A6%E3%81%A7%E3%82%84%E3%81%A3%E3%81%9F%E3%81%82%E3%82%8C%E3%81%A7%E3%81%99%E3%80%82%E5%83%95%E3%81%AF%E5%B0%82%E6%94%BB%E3%81%8C%E5%88%86%E5%AD%90%E7%94%9F%E6%85%8B%E5%AD%A6%E3%81%A7%E5%B0%82%E9%96%80%E5%88%86%E9%87%8E%E3%81%A7%E3%81%AF%E3%81%AA%E3%81%84%E3%81%A7%E3%81%99%E3%81%8C%E3%80%81%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%A8%E3%81%AF%E7%A5%9E%E7%B5%8C%E7%B4%B0%E8%83%9E%E3%81%AE%E3%81%93%E3%81%A8%E3%81%A7%E3%80%81%E6%A8%B9%E7%8A%B6%E3%81%AE%E7%AA%81%E8%B5%B7%E3%81%8B%E3%82%89%E5%88%BA%E6%BF%80%E3%82%92%E5%8F%97%E3%81%91%E3%80%81%E8%BB%B8%E7%B4%A2%E3%81%8B%E3%82%89%E4%BB%96%E3%81%AE%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%AB%E5%88%BA%E6%BF%80%E3%82%92%E4%BC%9D%20%E9%81%94%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。この「入力刺激の統合と発火」という仕組みを模倣したのが**パーセプトロン**（単純パーセプトロン）と呼ばれるアルゴリズムです。パーセプトロンは**生物学的ニューロンを単純化したモデル**であり ([Perceptron - Wikipedia](https://en.wikipedia.org/wiki/Perceptron#:~:text=The%20perceptron%20is%20a%20simplified,30))、複数の入力を受け取って一つの出力を生成します。例えば2入力1出力のパーセプトロンでは、入力信号 \(x_1, x_2\) に重み \(w_1, w_2\) を掛け合わせ、さらにバイアス項 \(w_0\)（発火のしやすさを調整する定数項）を加算して内部変数 \(y'\) を計算します ([パーセプトロンの仕組みや用語について解説 ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%AF%E5%85%A5%E5%8A%9B%20x%201%2Cx%202%20%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6,1%2Bw%202%20x%202%20%E3%82%92%E8%A8%88%E7%AE%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。数式で表せば次のようになります：

\[ y' = w_0 + w_1 x_1 + w_2 x_2 \]

この \(y'\) に対し、**活性化関数**として閾値処理を適用し、例えば「\(y' \ge 0\) なら出力 \(y=1\)、\(y' < 0\) なら \(y=0\)」というルールで出力 \(y\) を決定します ([パーセプトロンの仕組みや用語について解説 ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%AF%E5%85%A5%E5%8A%9B%20x%201%2Cx%202%20%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6,1%2Bw%202%20x%202%20%E3%82%92%E8%A8%88%E7%AE%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。このようにパーセプトロンは入力の線形結合をとり、閾値を境に出力が離散的に変化するモデルです。パーセプトロンは1957年にフランク・ローゼンブラットによって提案され、「**脳神経を模して人間と同じ認識能力を再現しよう**」とする試みでした ([パーセプトロンの仕組みや用語について解説 ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%A8%E3%81%AF))。

パーセプトロンでは、複数の入力に対応する**重み** \(w_i\) と**バイアス** \(w_0\) がパラメータとなります。このパラメータの値によって、パーセプトロンが実現する分類の境界（判別関数）が決まります ([パーセプトロン・単純パーセプトロン・多層パーセプトロン(MLP) | AI研究所](https://ai-kenkyujo.com/term/perceptron-simple-perceptron-multilayer-perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%A7%E3%81%AF%E3%80%81%E8%A4%87%E6%95%B0%E3%81%AE%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%8B%E3%82%89%E3%81%AE%E5%87%BA%E5%8A%9B%E3%81%8C%E9%87%8D%E3%81%BF%E4%BB%98%E3%81%91%E3%81%95%E3%82%8C%E3%80%81%E3%81%9D%E3%81%AE%E7%B7%9A%E5%BD%A2%E5%92%8C%E3%81%A8%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E9%A0%85%E3%81%AE%E7%B7%8F%E5%92%8C%E3%81%8C%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%AB%E4%BC%9D%E9%81%94%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%20%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%A8%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%8C%E5%B1%A4%E3%81%A7%E6%A7%8B%E6%88%90%E3%81%95%E3%82%8C%E3%82%8B%E3%82%82%E3%81%AE%E3%82%92%20%E5%8D%98%E7%B4%94%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%80%81%E9%9A%A0%E3%82%8C%E5%B1%A4%E3%82%92%E5%90%AB%E3%82%80%E4%B8%89%E5%B1%A4%E4%BB%A5%E4%B8%8A%E3%81%AB%E5%A4%9A%E5%B1%A4%E5%8C%96%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%82%82%E3%81%AE%E3%81%8C%20%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3))。単純パーセプトロンは入力層と出力層のみを持つ二層構造ですが、これに**隠れ層（中間層）**を加えて多層化したものが後述する多層ニューラルネットワーク、すなわち**多層パーセプトロン (MLP)** です ([パーセプトロン・単純パーセプトロン・多層パーセプトロン(MLP) | AI研究所](https://ai-kenkyujo.com/term/perceptron-simple-perceptron-multilayer-perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%A7%E3%81%AF%E3%80%81%E8%A4%87%E6%95%B0%E3%81%AE%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%8B%E3%82%89%E3%81%AE%E5%87%BA%E5%8A%9B%E3%81%8C%E9%87%8D%E3%81%BF%E4%BB%98%E3%81%91%E3%81%95%E3%82%8C%E3%80%81%E3%81%9D%E3%81%AE%E7%B7%9A%E5%BD%A2%E5%92%8C%E3%81%A8%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E9%A0%85%E3%81%AE%E7%B7%8F%E5%92%8C%E3%81%8C%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%AB%E4%BC%9D%E9%81%94%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%20%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%A8%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%8C%E5%B1%A4%E3%81%A7%E6%A7%8B%E6%88%90%E3%81%95%E3%82%8C%E3%82%8B%E3%82%82%E3%81%AE%E3%82%92%20%E5%8D%98%E7%B4%94%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%80%81%E9%9A%A0%E3%82%8C%E5%B1%A4%E3%82%92%E5%90%AB%E3%82%80%E4%B8%89%E5%B1%A4%E4%BB%A5%E4%B8%8A%E3%81%AB%E5%A4%9A%E5%B1%A4%E5%8C%96%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%82%82%E3%81%AE%E3%81%8C%20%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3))。

### 活性化関数とその役割
**活性化関数**（Activation Function）とは、パーセプトロン（人工ニューロン）の線形和 \(y'\) に対して出力を非線形に変換する関数 \(h(y')\) のことです ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%A8%E3%81%AF))。活性化関数を導入することでモデルの表現力が飛躍的に向上します。線形和をそのまま出力するだけではモデルは線形な入出力しか表現できませんが、**非線形な活性化関数で変換したニューロンを組み合わせればより複雑な関数も表現可能**になります ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%82%92%E7%94%A8%E3%81%84%E3%82%8B%E3%81%A8%20%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%A1%A8%E7%8F%BE%E5%8A%9B%20%E3%81%8C%E5%A2%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。

パーセプトロンで用いられる活性化関数として代表的なものに以下があります：

- **ステップ関数（閾値関数）**：ある閾値を境に出力が0か1のどちらかになる関数です。パーセプトロンの元々の形式で使われ、ニューロンの発火/不発火を0/1でモデル化します ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=h%28y%E2%80%B2%29%3D%7B1%200%28y%E2%80%B2%C2%A0%E2%89%A50%29%28y%E2%80%B2))。シンプルですが出力が離散値しか取れず、微分不可能な点があるため学習には不向きです。
- **シグモイド関数**：S字型の連続な活性化関数で、実数入力を0から1の範囲の出力にマッピングします ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=h))。形式は \( \sigma(y') = \frac{1}{1 + \exp(-y')} \) で表され、ニューロンの出力を確率的解釈できるためかつては広く用いられました ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0%E3%82%92%E7%94%A8%E3%81%84%E3%82%8B%E3%81%A80%E3%81%8B%E3%82%891%E3%81%BE%E3%81%A7%E9%80%A3%E7%B6%9A%E3%81%AA%E5%80%A4%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%99%E3%82%8B%E3%81%93%E3%81%A8%E3%81%8C%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82))。しかし、大きな入力では勾配（微分値）が0に近づき**勾配消失**を起こしやすいため、現在では隠れ層の活性化としてはあまり使われません ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0%20%E3%81%AF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A7%E7%94%A8%E3%81%84%E3%82%89%E3%82%8C%E3%82%8B%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%A7%E3%81%99%E3%81%8C%E3%80%81%E7%8F%BE%E5%9C%A8%E3%81%AF%E3%81%82%E3%81%BE%E3%82%8A%E4%BD%BF%E3%82%8F%E3%82%8C%E3%81%A6%E3%81%84%E3%81%BE%E3%81%9B%E3%82%93%E3%80%82))。
- **ハイパボリックタンジェント（tanh）関数**：出力範囲が -1 から 1 になるシグモイドの派生関数です。数式は \( \tanh(y') = \frac{\exp(y') - \exp(-y')}{\exp(y') + \exp(-y')} \) で、0中心の出力を持つためシグモイドより学習が安定しやすいですが、やはり大きな値で勾配が小さくなる欠点があります。
- **ReLU関数**：**Rectified Linear Unit**の略で、現在最も一般的に使われる活性化関数です ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=ReLU%E9%96%A2%E6%95%B0))。入力が正のときはそのまま出力し、負のときは0を出力する単純な関数で、式で書くと \( \mathrm{ReLU}(y') = \max(0,\,y') \) です。そのグラフは原点を境に折れ線状になります。ReLUは負の領域で勾配が0になるものの、シグモイドのように全域で勾配が小さくならないため**勾配消失が起こりにくく**、計算も高速という利点があります ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=ReLU%E9%96%A2%E6%95%B0))。このためディープなネットワークでも学習が進みやすく、現在の深層学習では隠れ層の活性化関数として標準的に用いられています。

他にもReLUを改良した**Leaky ReLU**（負の入力もわずかに傾きあり）や、近年提案された**Swish**、**Mish**など様々な活性化関数がありますが、基本的な役割は「**ネットワークに非線形性を導入し、モデルに複雑なパターンを学習させる**」ことにあります ([活性化関数(activation function) 【深層学習向け】](https://cvml-expertguide.net/terms/dl/layers/activation-function/#:~:text=%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0,%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%81%AF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AE%E9%9A%A0%E3%82%8C%E5%B1%A4%E3%82%82%E3%81%97%E3%81%8F%E3%81%AF%E5%87%BA%E5%8A%9B%E3%81%AB%E3%81%8A%E3%81%84%E3%81%A6%EF%BC%8C%E3%80%8C%E6%8A%BC%E3%81%97%E3%81%A4%E3%81%B6%E3%81%97%EF%BC%88squashing%EF%BC%89%E3%80%8D%E3%81%AB%E3%82%88%E3%82%8B%E5%87%BA%E5%8A%9B%E7%AF%84%E5%9B%B2%E3%81%AE%E8%AA%BF%E6%95%B4%EF%BC%8C%E3%81%8A%E3%82%88%E3%81%B3%E9%9D%9E%E7%B7%9A%E5%BD%A2%E5%A4%89%E6%8F%9B%E3%81%AB%E3%82%88%E3%82%8B%E8%A1%A8%E7%8F%BE%E5%8A%9B%E3%81%AE%E5%90%91%E4%B8%8A)) ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0%E3%82%92%E7%94%A8%E3%81%84%E3%82%8B%E3%81%A8%20%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E8%A1%A8%E7%8F%BE%E5%8A%9B%20%E3%81%8C%E5%A2%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。また出力層にもタスクに応じた活性化関数が用いられます。例えば二値分類ではシグモイド関数、多クラス分類では各クラスの確率を出力する**ソフトマックス関数**、回帰問題では制限をかけない恒等関数などを使い分けます。適切な活性化関数を選ぶことはニューラルネットワークの性能に大きく影響します。

### 多層ニューラルネットワーク
**多層ニューラルネットワーク**（Multi-Layer Neural Network）とは、パーセプトロンを縦積みにして隠れ層を持たせたネットワーク構造です。最も基本的な形式は**多層パーセプトロン (MLP)** と呼ばれ、入力層と出力層の間に1つ以上の**隠れ層（Hidden Layer）**を配置します ([パーセプトロン・単純パーセプトロン・多層パーセプトロン(MLP) | AI研究所](https://ai-kenkyujo.com/term/perceptron-simple-perceptron-multilayer-perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%A7%E3%81%AF%E3%80%81%E8%A4%87%E6%95%B0%E3%81%AE%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%8B%E3%82%89%E3%81%AE%E5%87%BA%E5%8A%9B%E3%81%8C%E9%87%8D%E3%81%BF%E4%BB%98%E3%81%91%E3%81%95%E3%82%8C%E3%80%81%E3%81%9D%E3%81%AE%E7%B7%9A%E5%BD%A2%E5%92%8C%E3%81%A8%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E9%A0%85%E3%81%AE%E7%B7%8F%E5%92%8C%E3%81%8C%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%AB%E4%BC%9D%E9%81%94%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%20%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%A8%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%8C%E5%B1%A4%E3%81%A7%E6%A7%8B%E6%88%90%E3%81%95%E3%82%8C%E3%82%8B%E3%82%82%E3%81%AE%E3%82%92%20%E5%8D%98%E7%B4%94%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%80%81%E9%9A%A0%E3%82%8C%E5%B1%A4%E3%82%92%E5%90%AB%E3%82%80%E4%B8%89%E5%B1%A4%E4%BB%A5%E4%B8%8A%E3%81%AB%E5%A4%9A%E5%B1%A4%E5%8C%96%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%82%82%E3%81%AE%E3%81%8C%20%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3))。各層のニューロンは前の層の出力を入力とし、重み付き和と活性化関数による変換を経て次の層へ信号を伝達します。このように層を重ねることで、ニューラルネットワークは**階層的な特徴抽出**を行えるようになります。入力に対して一次的な特徴を隠れ層で抽出し、さらに次の隠れ層でその組み合わせからより高次の特徴を抽出するといった具合に、徐々に高度なパターンを学習します。

単層のパーセプトロンは線形分離可能な問題しか解けませんが、多層ネットワークにより非線形な判別も可能になります。その理論的背景として、十分なニューロン数を持つ隠れ層が1層あれば任意の連続関数を近似できることが知られており、これは**「万能近似定理」**と呼ばれます（1989年、Cybenkoらによる証明）。すなわち、多層ニューラルネットワークは理論上は非常に複雑な入出力関係も表現可能な**強力な汎用関数近似器**です ([パーセプトロン・単純パーセプトロン・多層パーセプトロン(MLP) | AI研究所](https://ai-kenkyujo.com/term/perceptron-simple-perceptron-multilayer-perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%A7%E3%81%AF%E3%80%81%E8%A4%87%E6%95%B0%E3%81%AE%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%8B%E3%82%89%E3%81%AE%E5%87%BA%E5%8A%9B%E3%81%8C%E9%87%8D%E3%81%BF%E4%BB%98%E3%81%91%E3%81%95%E3%82%8C%E3%80%81%E3%81%9D%E3%81%AE%E7%B7%9A%E5%BD%A2%E5%92%8C%E3%81%A8%E3%83%90%E3%82%A4%E3%82%A2%E3%82%B9%E9%A0%85%E3%81%AE%E7%B7%8F%E5%92%8C%E3%81%8C%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%BA%E5%B7%A5%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%AD%E3%83%B3%E3%81%AB%E4%BC%9D%E9%81%94%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%20%E5%85%A5%E5%8A%9B%E5%B1%A4%E3%81%A8%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AE%E4%BA%8C%E5%B1%A4%E3%81%A7%E6%A7%8B%E6%88%90%E3%81%95%E3%82%8C%E3%82%8B%E3%82%82%E3%81%AE%E3%82%92%20%E5%8D%98%E7%B4%94%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%80%81%E9%9A%A0%E3%82%8C%E5%B1%A4%E3%82%92%E5%90%AB%E3%82%80%E4%B8%89%E5%B1%A4%E4%BB%A5%E4%B8%8A%E3%81%AB%E5%A4%9A%E5%B1%A4%E5%8C%96%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%82%82%E3%81%AE%E3%81%8C%20%E5%A4%9A%E5%B1%A4%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3))。

もっとも、隠れ層を増やせば自動的に精度が向上するわけではありません。層を重ねるとパラメータ（重み）の数が増大し、適切に学習させるのが難しくなる問題（後述する勾配消失や過学習など）が発生します。そのため1980～90年代には多層ネットの学習が困難でしたが、2006年頃に層ごとの事前学習や活性化関数の工夫、大規模データセットの出現やGPUによる高速計算などのブレイクスルーによって「ディープラーニング」として多層ニューラルネットワークが再注目されるようになりました。現在では数十層にも及ぶディープなネットワークが画像認識や自然言語処理で高い性能を発揮しています。

まとめると、多層ニューラルネットワークは入力層・**隠れ層**（複数）・出力層から構成される人工の神経回路であり、活性化関数による非線形変換を各層で行うことで複雑な入出力マッピングを学習できるモデルです。次に、その学習（パラメータ最適化）の仕組みについて説明します。

### 損失関数と勾配降下法
ニューラルネットワークを含む機械学習モデルの学習では、モデルの予測結果と正解とのズレを定量化した**損失関数（誤差関数）**を最小化するようにパラメータ（重みとバイアス）を更新します。損失関数 \(L\) とはモデルの性能の悪さを表す指標で、パラメータの関数として定義されます。タスクに応じて様々な損失関数が使われますが、典型例として以下があります：

- **二乗和誤差（平均二乗誤差）**：回帰問題でよく用いられ、予測値 \(y\) と目標値 \(t\) の差の二乗により誤差を測ります。1データあたりの二乗和誤差は \(L = \frac{1}{2}(t - y)^2\) などと定義され、これは差分の大きい外れ値に対して特に大きなペナルティを与えます ([ニューラルネットワークの基礎 — ディープラーニング入門：Chainer チュートリアル](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html#ニューラルネットワークの訓練#:~:text=%5C%5BL%20%3D%20%5Cdfrac%7B1%7D%7BN%7D%20%5Csum_%7Bn%3D1%7D,2))。複数データの場合は全データでの平均 \( \frac{1}{N}\sum_n (t_n - y_n)^2 \) をとります ([ニューラルネットワークの基礎 — ディープラーニング入門：Chainer チュートリアル](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html#ニューラルネットワークの訓練#:~:text=%5C%5BL%20%3D%20%5Cdfrac%7B1%7D%7BN%7D%20%5Csum_%7Bn%3D1%7D,2))。
- **クロスエントロピー損失**：分類問題で広く使われます。モデルが出力する確率分布と実際の正解ラベル（の分布）との間のエントロピー（情報量）を計算したものです。例えば正解ラベルを示すone-hotベクトル \(t\) と予測確率 \(y\) に対し、多クラス分類のクロスエントロピーは \(L = -\sum_{k} t_k \log y_k\) と表されます。これは「正解クラスの予測確率の対数」にマイナスを付けたもので、正解の確率が高いほど損失が小さくなり、正解に自信がなければ損失が大きくなります。

損失関数 \(L(\Theta)\) はパラメータ全体（ベクトル \(\Theta\)）の関数です。**学習**ではこの損失関数をできるだけ小さくするパラメータを探索しますが、直接的に最小値を解析的に解くことは困難です。そこで用いられるのが**勾配降下法（Gradient Descent）**と呼ばれる最適化アルゴリズムです。勾配降下法では、「現在のパラメータにおける損失関数の勾配（傾き）」を計算し、その勾配が示す**増加方向の反対向き（すなわち損失を減らす方向）**にパラメータを少し動かす操作を繰り返します ([〖ディープラーニング入門（第4回）〗勾配降下法を学んでディープラーニングの学習について理解しよう #DeepLearning - Qiita](https://qiita.com/kwi0303/items/7bfd7180f80a52296e64#:~:text=1))。勾配とは多変数関数の各パラメータ偏微分を並べたベクトルで、「パラメータをどの方向にどれだけ微小変化させれば損失が増減するか」を示すものです。勾配降下法では勾配 \(\nabla_{\Theta} L\) を用いて、例えば次のようにパラメータを更新します：

\[ \Theta \leftarrow \Theta - \eta \, \nabla_{\Theta} L, \]

ここで \(\eta\) は**学習率（Learning Rate）**と呼ばれるハイパーパラメータで、勾配に沿って移動するステップ幅を決めます。学習率が大きすぎると最適値を飛び越えて発散し、小さすぎると収束に非常に時間がかかるため、適切な値を設定することが重要です。

勾配降下法の直感的な理解としては、損失関数のパラメータ空間における地形（誤差の山や谷）を想像し、現在位置から**最も下り坂となる方向**に進んでいく手続きをイメージすると分かりやすいでしょう ([〖ディープラーニング入門（第4回）〗勾配降下法を学んでディープラーニングの学習について理解しよう #DeepLearning - Qiita](https://qiita.com/kwi0303/items/7bfd7180f80a52296e64#:~:text=1))。勾配（傾き）がゼロになる地点（谷の底）が見つかれば、それが局所的に損失を最小にするパラメータです。

実際の学習では、すべての訓練データに対して損失を計算しその勾配を求める**バッチ勾配降下法**よりも、データを小さなグループ（ミニバッチ）に分けて順次勾配降下を行う**ミニバッチ確率的勾配降下法（SGD）**が一般的です。ミニバッチSGDでは各更新ステップで一部のデータだけを見るため多少の誤差がありますが、計算効率が高く大規模データでも扱え、またノイズのおかげで局所解から抜け出しやすいという利点もあります ([〖ディープラーニング入門（第4回）〗勾配降下法を学んでディープラーニングの学習について理解しよう #DeepLearning - Qiita](https://qiita.com/kwi0303/items/7bfd7180f80a52296e64#:~:text=1,%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB))。1件ずつ更新する極端な場合は**オンライン学習（確率的勾配降下法）**と呼ばれます。

このようにして勾配降下法によりパラメータを徐々に調整し、損失を減らしていくことでニューラルネットワークは訓練データに適合するよう学習します。では、ニューラルネットワークの勾配（偏微分）は具体的にどのように計算されるのでしょうか？ それを可能にするのが次に述べる**誤差逆伝播法**（バックプロパゲーション）です。

### 逆伝播アルゴリズムの直感的理解
**誤差逆伝播法（バックプロパゲーション、Backpropagation）**は、多層ニューラルネットワークの重みパラメータに関する勾配を効率よく計算するためのアルゴリズムです ([什么是反向传播？| IBM](https://www.ibm.com/cn-zh/think/topics/backpropagation#:~:text=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%98%AF%E4%B8%80%E7%A7%8D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E6%8A%80%E6%9C%AF%EF%BC%8C%E5%AF%B9%E4%BC%98%E5%8C%96%201%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E3%80%82%E5%AE%83%E6%9C%89%E5%8A%A9%E4%BA%8E%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%EF%BC%8C%E8%BF%99%E5%B0%B1%E6%98%AF%203%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%8A%A8%E7%8E%B0%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA,AI%29%20%E2%80%9C%E5%AD%A6%E4%B9%A0%E2%80%9D%E7%9A%84%E6%96%B9%E5%BC%8F%E3%80%82))。バックプロパゲーションでは、出力層で計算された損失（誤差）を各重みに対する勾配に変換しながらネットワークの入力方向へ**伝播（逆流）**させます。これは微分の**連鎖律（Chain Rule）**に基づく手法で、各層の出力が次の層の入力に影響を及ぼす関係を遡りながら、**局所的な勾配を順次掛け合わせていく**ことで最終的に各重みの勾配を得ます ([ニューラルネットワークの基礎 — ディープラーニング入門：Chainer チュートリアル](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html#ニューラルネットワークの訓練#:~:text=)) ([什么是反向传播？| IBM](https://www.ibm.com/cn-zh/think/topics/backpropagation#:~:text=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E9%80%BB%E8%BE%91%E6%98%AF%EF%BC%8C%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E5%B1%82%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%98%AF%E4%B8%80%E7%B3%BB%E5%88%97%E5%B5%8C%E5%A5%97%E7%9A%84%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0%E3%80%82%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%9B%B8%E4%BA%92%E5%85%B3%E8%81%94%E7%9A%84%E6%96%B9%E7%A8%8B%E8%A2%AB%E5%B5%8C%E5%A5%97%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%E4%B8%AD%EF%BC%9A%E2%80%9C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E2%80%9D%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%B5%8B%E9%87%8F%E7%BB%99%E5%AE%9A%E8%BE%93%E5%85%A5%E7%9A%84%E6%9C%9F%20%E6%9C%9B%E8%BE%93%E5%87%BA%EF%BC%88%E6%88%96%E2%80%9C%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E2%80%9D%EF%BC%89%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%99%85%E8%BE%93%E5%87%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%EF%BC%88%E6%88%96%E2%80%9C%E6%8D%9F%E5%A4%B1%E2%80%9D%EF%BC%89%E3%80%82))。

直感的には、バックプロパゲーションは「出力の誤差の原因を各重みに配分していく」作業と考えられます。まず出力層の誤差（例えば「予測と正解の差」）から、出力層直前の重みがどれだけその誤差に寄与したかを計算します。次に、その寄与（残差）をさらに一つ手前の層にさかのぼり、今度は前の層の重みの勾配を計算します。この過程を入力側へ繰り返し伝播させることで、すべての重みについて損失勾配を求めることができます。**計算量は各層の順伝播と同程度**で済み、層数に線形なオーダーで効率良く勾配を算出できます。バックプロパゲーションによって得られた各重みの勾配を使い、前述の勾配降下法でパラメータを更新すれば学習が進みます。

重要なのは、バックプロパゲーションそのものは**学習アルゴリズムではなく勾配計算の手順**である点です ([什么是反向传播？| IBM](https://www.ibm.com/cn-zh/think/topics/backpropagation#:~:text=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%98%AF%E4%B8%80%E7%A7%8D%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%20%E6%8A%80%E6%9C%AF%EF%BC%8C%E5%AF%B9%E4%BC%98%E5%8C%96%201%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E3%80%82%E5%AE%83%E6%9C%89%E5%8A%A9%E4%BA%8E%E4%BD%BF%E7%94%A8%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%20%E7%AE%97%E6%B3%95%E6%9B%B4%E6%96%B0%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%EF%BC%8C%E8%BF%99%E5%B0%B1%E6%98%AF%203%E6%A8%A1%E5%9E%8B%E6%8E%A8%E5%8A%A8%E7%8E%B0%E4%BB%A3%E4%BA%BA%E5%B7%A5%E6%99%BA,AI%29%20%E2%80%9C%E5%AD%A6%E4%B9%A0%E2%80%9D%E7%9A%84%E6%96%B9%E5%BC%8F%E3%80%82))。学習アルゴリズムである勾配降下法（SGDなど）は、このバックプロパゲーションで計算された勾配を用いて重みを調整します。逆に言えば、バックプロパゲーションにより**任意の重みやバイアスを変化させたとき損失がどのように変化するか（予測精度に与える影響）を効率的に計算**できるので ([什么是反向传播？| IBM](https://www.ibm.com/cn-zh/think/topics/backpropagation#:~:text=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E6%98%AF%E2%80%9C%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E2%80%9D%E7%9A%84%E7%AE%80%E5%86%99%EF%BC%8C%E5%AE%83%E6%98%AF%E4%B8%80%E7%A7%8D%E4%BC%98%E9%9B%85%E6%96%B9%E6%B3%95%EF%BC%8C%E5%8F%AF%E7%94%A8%E4%BA%8E%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BB%BB%E6%84%8F%E6%9D%83%E9%87%8D%E6%88%96%E5%81%8F%E5%B7%AE%E7%9A%84%E5%8F%98%E5%8C%96%E4%BC%9A%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B%E7%9A%84%E5%87%86%E7%A1%AE%E6%80%A7%E3%80%82%E5%AE%83%E5%AF%B9%E4%BA%8E%E4%BD%BF%E7%94%A8%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E3%80%81%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%20%E6%88%96%20%206%E6%9D%A5%E8%AE%AD%E7%BB%83%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%87%B3%E5%85%B3%E9%87%8D%E8%A6%81%E3%80%82))、大規模なネットワークでも現実的な時間で勾配降下法を適用できるのです。

バックプロパゲーションの具体例として、先ほどの2層ネットワーク（入力層－隠れ層－出力層）のケースを考えてみましょう。まず出力層の各重みについて、損失の出力に関する勾配 \(\partial L/\partial y\) と出力の重みに関する勾配 \(\partial y/\partial w\) を掛け合わせて \(\partial L/\partial w\) を得ます ([ニューラルネットワークの基礎 — ディープラーニング入門：Chainer チュートリアル](https://tutorials.chainer.org/ja/13_Basics_of_Neural_Networks.html#ニューラルネットワークの訓練#:~:text=%E3%81%9D%E3%82%8C%E3%81%A7%E3%81%AF%E3%81%BE%E3%81%9A%E3%80%81%E5%87%BA%E5%8A%9B%E5%B1%A4%E3%81%AB%E8%BF%91%E3%81%84%E6%96%B9%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%80%81%5C%28,%E3%81%93%E3%82%8C%E3%81%AF%E3%80%81%E5%90%88%E6%88%90%E9%96%A2%E6%95%B0%E3%81%AE%E5%81%8F%E5%BE%AE%E5%88%86%E3%81%AA%E3%81%AE%E3%81%A7%E3%80%81%E9%80%A3%E9%8E%96%E5%BE%8B%EF%BC%88chain%20rule%EF%BC%89%E3%82%92%E7%94%A8%E3%81%84%E3%81%A6%E4%BB%A5%E4%B8%8B%E3%81%AE%E3%82%88%E3%81%86%E3%81%AB%E5%B1%95%E9%96%8B%E3%81%A7%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82))。次に、隠れ層の重みについては、隠れ層の出力が最終出力を経由して損失に与える勾配（連鎖律で \(\partial L/\partial h \cdot \partial h/\partial w\) のような形）を計算します。このように順に伝播させることで全ての勾配が求まります。実装上は、各ニューロンの出力に対する損失の勾配を保持し（これを「デルタ」と呼ぶこともあります）、それを前層へ線形変換で伝えていく形になります。

現在の深層学習フレームワーク（例えばPyTorchやTensorFlowなど）では、バックプロパゲーションは自動的に行われるのでユーザが個々の勾配計算式を導出する必要はありません。しかしその仕組みを理解しておくことは、ネットワーク設計やトラブルシューティングに役立ちます。逆伝播アルゴリズムの要点は「**出力の誤差情報をネットワーク内で逆向きに伝え、各パラメータの勾配を効率よく計算する**」ことにあります ([什么是反向传播？| IBM](https://www.ibm.com/cn-zh/think/topics/backpropagation#:~:text=%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E9%80%BB%E8%BE%91%E6%98%AF%EF%BC%8C%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%A5%9E%E7%BB%8F%E5%85%83%E5%B1%82%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%98%AF%E4%B8%80%E7%B3%BB%E5%88%97%E5%B5%8C%E5%A5%97%E7%9A%84%E6%95%B0%E5%AD%A6%E5%87%BD%E6%95%B0%E3%80%82%E5%9C%A8%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%AD%EF%BC%8C%E8%BF%99%E4%BA%9B%E7%9B%B8%E4%BA%92%E5%85%B3%E8%81%94%E7%9A%84%E6%96%B9%E7%A8%8B%E8%A2%AB%E5%B5%8C%E5%A5%97%E5%88%B0%E5%8F%A6%E4%B8%80%E4%B8%AA%E5%87%BD%E6%95%B0%E4%B8%AD%EF%BC%9A%E2%80%9C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E2%80%9D%EF%BC%8C%E7%94%A8%E4%BA%8E%E6%B5%8B%E9%87%8F%E7%BB%99%E5%AE%9A%E8%BE%93%E5%85%A5%E7%9A%84%E6%9C%9F%20%E6%9C%9B%E8%BE%93%E5%87%BA%EF%BC%88%E6%88%96%E2%80%9C%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E2%80%9D%EF%BC%89%E4%B8%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AE%9E%E9%99%85%E8%BE%93%E5%87%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%B7%AE%E5%BC%82%EF%BC%88%E6%88%96%E2%80%9C%E6%8D%9F%E5%A4%B1%E2%80%9D%EF%BC%89%E3%80%82))。

### 実践演習：手書き数字認識
最後に、本章で学んだニューラルネットワークの基礎を踏まえて**手書き数字認識**の簡単なモデルを実装してみましょう。これは有名なMNISTデータセット（0～9の手書き数字画像と正解ラベルからなるデータ）を使った多クラス分類問題です。入力は28×28ピクセルのグレースケール画像、出力は0～9のいずれかのクラスになります。

ここではPyTorchを用いて、多層ニューラルネットワーク（多層パーセプトロン）による手書き数字分類器を構築し、学習させてみます。まず必要なライブラリをインポートし、MNISTデータを用意します（PyTorchの `torchvision.datasets` から簡単に取得できます）。モデルは隠れ層を1層持つシンプルな全結合ネットワーク（入力784次元→隠れ層128次元→出力10次元）とします。活性化関数にはReLU、出力には各クラスの確率を表すソフトマックスに対応したクロスエントロピー損失を用います。以下にコード例を示します。

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms

# MNISTデータセットの読み込み（トレーニング用）
train_dataset = datasets.MNIST(root='./data', train=True, download=True,
                               transform=transforms.ToTensor())
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

# シンプルな多層パーセプトロンモデルの定義
class SimpleMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(28*28, 128)   # 隠れ層（128ユニット）
        self.fc2 = nn.Linear(128, 10)     # 出力層（10クラス）
    def forward(self, x):
        x = x.view(-1, 28*28)            # 画像をフラット化 (batch_size × 784)
        x = torch.relu(self.fc1(x))      # 隠れ層の線形変換＋ReLU活性化
        x = self.fc2(x)                  # 出力層の線形変換（活性化は損失関数内部で）
        return x

model = SimpleMLP()

# 損失関数と最適化手法の定義
criterion = nn.CrossEntropyLoss()         # クロスエントロピー損失（softmaxを含む）
optimizer = optim.SGD(model.parameters(), lr=0.1)
```

上記でモデルと学習の準備が整いました。次に学習ループを実行します。エポック（データセットを一巡する単位）を決め、トレーニングデータをミニバッチに分けて順次モデルに入力、損失を計算して勾配をバックプロパゲーションで求め、オプティマイザで重みを更新します。以下は3エポック学習させるループの例です。

```python
# 学習ループの実行（3エポック）
for epoch in range(3):
    total_loss = 0.0
    for images, labels in train_loader:
        optimizer.zero_grad()             # 勾配をリセット
        outputs = model(images)           # 順伝播（フォワードパス）
        loss = criterion(outputs, labels) # 損失を計算
        loss.backward()                   # 誤差逆伝播で勾配計算
        optimizer.step()                  # 勾配降下法でパラメータ更新
        total_loss += loss.item()
    avg_loss = total_loss / len(train_loader)
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")
```

実行すると各エポックで損失が徐々に減少していくのが確認できるはずです。学習が完了したモデルにテスト用の画像を与えると、たとえば入力画像が「7」のときモデルの予測も「7」になる、といった具合に手書き数字を高い精度で分類できるようになります。シンプルな全結合ネットワークでもMNIST程度であれば高い精度（90%以上）を達成できますが、より難しい画像認識では次章で扱う**畳み込みニューラルネットワーク**を用いるのが一般的です。

以上、第4章ではニューラルネットワークの基礎としてパーセプトロンから多層ネットワーク、そしてその学習アルゴリズムである誤差逆伝播法と勾配降下法について解説しました。

## 第5章：深層学習フレームワーク

### PyTorchの基本構造
深層学習の実装には便利なフレームワークが数多く存在しますが、本章ではFacebook社によって開発された**PyTorch**を取り上げます。PyTorchは動的な計算グラフと直感的なPythonインターフェースを特徴とするディープラーニングフレームワークで、研究からプロダクションまで幅広く利用されています。まずはPyTorchの基本的な構成要素を押さえましょう。

**Tensor（テンソル）**：PyTorchではデータは基本的にテンソルと呼ばれる多次元配列オブジェクトで扱われます。テンソルはNumPyのndarrayに似た構造ですが、PyTorchのテンソルはGPU上での計算や自動微分に対応している点が異なります。例えば:

```python
import torch
x = torch.tensor([[1.0, 2.0],
                  [3.0, 4.0]], requires_grad=True)
print(x.size())        # => torch.Size([2, 2])
```

このように2×2の行列を表すテンソルを生成できます。`requires_grad=True` とすると、このテンソルに対する演算で**自動微分（後述）**のための勾配情報を追跡するようになります。テンソルはCPUとGPUのメモリ上に配置でき、`.to(device)` メソッドでデバイス間を転送可能です。

**モジュール（nn.Module）**：PyTorchではニューラルネットワークモデルは `torch.nn.Module` クラスを継承して作成します。`nn.Module` はネットワークの層（レイヤー）をカプセル化したクラスで、内部にパラメータ（重みやバイアス）を持ち、`forward()` メソッドに順伝播の計算（入力から出力を得る処理）を実装します。これによりモデルの構造をオブジェクト指向的に定義できます。また、PyTorchは有名なモデル（ResNetやBERTなど）の定義と学習済みパラメータを `torchvision.models` や `torch.hub` から簡単に利用できるようにもしています。

**損失関数と最適化**：`torch.nn` モジュールには損失関数も定義されています。例えば平均二乗誤差なら `nn.MSELoss()`, クロスエントロピー損失なら `nn.CrossEntropyLoss()` といったクラスが用意されています。**最適化アルゴリズム**については `torch.optim` パッケージにSGDやAdamなどのオプティマイザが実装されています。これらはモデルのパラメータを受け取り、`step()` メソッドでパラメータ更新を行います。

**データローダ**：PyTorchにはデータセットを扱いやすくするための `torch.utils.data.Dataset` と `DataLoader` クラスがあります。`Dataset` はデータセットを抽象化するクラスで、PyTorchは画像やテキストなど様々な既存データセットのクラス（MNIST, CIFAR-10, ImageNet等）を用意しています。またカスタムデータセットも簡単に作成できます。`DataLoader` は `Dataset` をラップしてミニバッチ単位でデータを供給するイテレータを提供します。これにより大規模データでもミニバッチ学習やシャッフル、マルチスレッド読み込みなどが簡単に実現できます。

要約すると、PyTorchの基本構造は「**テンソル（データ表現）**」「**モジュール（モデル表現）**」「**オプティマイザと損失関数（学習のアルゴリズム部）**」「**データローダ（データ供給）**」という要素から成り立っています。次節以降でこれらを使った実際のモデル学習プロセスを見ていきます。

### 計算グラフと自動微分
PyTorchの大きな特徴の一つが、**動的計算グラフと自動微分（autograd）**です。これは先に述べたバックプロパゲーション（勾配計算）をフレームワークが自動でやってくれる仕組みのことです。

ニューラルネットの学習では損失の各パラメータに対する勾配を求める必要がありますが、PyTorchではテンソルに対する演算を追跡し、**計算グラフ**を構築することで勾配計算を自動化しています。`requires_grad=True` のテンソルに対して演算を行うと、その演算の計算グラフ上でのノード（頂点）と勾配計算に必要な情報（関数のヤコビアンなど）が保存されます。逆伝播を行うときは、この計算グラフを後ろ向きにたどりながら各ノードでの微分を自動的に適用し、最終的に各テンソルの勾配が求まります。

簡単な例で示します。以下ではスカラーのテンソル \(x\) に対し、関数 \(y = 2x^2 + 3\) を計算してから \(y\) を微分し、\(\frac{dy}{dx}\) を自動的に求めています。

```python
x = torch.tensor(3.0, requires_grad=True)
y = 2 * x**2 + 3          # y = 2x^2 + 3 を計算
y.backward()              # yをxで微分（バックプロパゲーション実行）
print(x.grad)             # xの勾配を表示
# tensor(12.)  （dy/dx = 4x なので x=3 のとき勾配は12）
```

上記のように、`tensor(12.)` という結果が得られます。このように**`Tensor.backward()`** を呼び出すと（注意: スカラーでないテンソルの場合は引数に勾配を渡す必要があります）、そのテンソルを**最終出力（損失）**とみなして各入力テンソルに対する勾配が自動計算され、対応するテンソルの `grad` 属性に格納されます。

PyTorchの計算グラフは**動的**である点も重要です。これは、各イテレーションで計算グラフを再構築するという意味です。TensorFlow（1.x系）などかつての静的グラフとは異なり、Pythonの制御フロー（if文やループ）をそのまま使ってグラフ構造を変化させることができます。これにより柔軟なモデル記述が可能で、デバッグもしやすくなっています。

自動微分によって、ユーザはもはや偏微分の計算式を一つひとつ手導きする必要がありません。**順伝播の計算さえ正しく記述すれば、PyTorchが内部で逆伝播を実行して勾配を計算してくれる**のです。これは研究開発の生産性を大いに高めました。ただし、自動微分機能を誤用するとメモリリークを招いたり、意図しない計算グラフを構築してしまうこともあります。例えば各イテレーションで勾配を蓄積しないように `optimizer.zero_grad()`（または`model.zero_grad()`）で勾配をリセットする、不要になった計算グラフは `detach()` や `with torch.no_grad():` ブロックで切り離す、など正しい使用法を心がける必要があります。

以上がPyTorchの自動微分機構の概要です。これにより複雑なニューラルネットでも勾配計算がワンタッチで可能になり、私たちはモデルの設計と高レベルな訓練ループの実装に専念できます。

### モデルの定義と学習ループ
次に、PyTorchでのモデルの定義方法と学習ループの書き方を説明します。ここでは例として前章の手書き数字認識と同様の多層パーセプトロンを扱いますが、畳み込みネットワークでも基本的な手順は共通です。

**モデルの定義（nn.Moduleの継承）**：PyTorchでは先述のとおり、`nn.Module` を継承したクラスとしてモデル（ニューラルネットワーク）を定義します。`__init__` コンストラクタでレイヤー（層）となるモジュールを作成し、`forward` メソッドでそのレイヤーを組み合わせた演算を記述します。以下に入力784次元、隠れ層100次元、出力10次元のMLPモデルの例を示します。

```python
import torch.nn as nn

class MyNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 100)  # 全結合層1: 784->100
        self.fc2 = nn.Linear(100, 10)   # 全結合層2: 100->10
    def forward(self, x):
        x = x.view(x.size(0), -1)      # バッチ内の各画像をフラット化
        x = torch.relu(self.fc1(x))    # 隠れ層の活性化にReLU
        x = self.fc2(x)                # 出力層（ソフトマックスは後で）
        return x
```

上記では2つの全結合層を持つネットワークを定義しました。使い方としては、インスタンス化してから入力テンソルを関数のように与えるだけです。

```python
model = MyNet()
out = model(input_tensor)  # forwardが自動で呼ばれて出力テンソルが得られる
```

**学習ループ**：モデルを定義したら、損失関数とオプティマイザを設定し、学習ループを回します。基本的な流れは以下の通りです。

1. **データローダ**からミニバッチを取得する。
2. **順伝播**でモデルに入力を与え、出力を計算する。
3. **損失の計算**：出力と正解から損失関数を適用してスカラーの損失値を得る。
4. **勾配の初期化**：前回ループの勾配が残っていればクリアする（`optimizer.zero_grad()`）。
5. **逆伝播**：`loss.backward()` を呼び、各パラメータの勾配を計算する。
6. **重み更新**：`optimizer.step()` を呼び、勾配降下法でパラメータを更新する。
7. 必要に応じて学習の進捗を記録・表示する（損失の平均や精度など）。

この1～6をエポック数だけ繰り返します。エポックとはデータセット全体を一通り学習に使う単位です。例えばエポック数10なら、訓練データ全件を使った更新を10回繰り返すことになります。

PyTorchでは一般にこの学習ループを**明示的**に書く必要があります（TensorFlowの高レベルAPIでは自動化も可能ですが、PyTorchは可読性と柔軟性のため手動で書くスタイルが主流です）。以下は簡単な学習ループの疑似コードです：

```python
model = MyNet()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

for epoch in range(num_epochs):
    model.train()                         # 訓練モードに切り替え（DropoutやBNがあれば）
    running_loss = 0.0
    for inputs, labels in train_loader:
        optimizer.zero_grad()             # 勾配をクリア
        outputs = model(inputs)           # 順伝播
        loss = criterion(outputs, labels) # 損失計算
        loss.backward()                   # 逆伝播（勾配計算）
        optimizer.step()                  # 重み更新
        running_loss += loss.item()
    avg_loss = running_loss / len(train_loader)
    print(f"Epoch[{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f}")
```

このようにしてモデルの学習が進んでいきます。`model.train()` は学習時の挙動に切り替えるおまじないで、逆に評価時は `model.eval()` を呼んでおくと、ドロップアウトやバッチ正規化の挙動が評価用（推論用）になります。

**評価と推論**：学習後、未知のデータに対して予測（推論）するには単に `model(test_input)` を実行します。ただし学習時のように勾配は不要なので、`with torch.no_grad():` コンテキスト内で `model(eval_input)` とすることで速度向上とメモリ節約ができます。また多クラス分類の場合、出力はスコア（対数確率）のままなので `torch.argmax(output, dim=1)` で最も高いスコアのインデックスを予測クラスとして取り出すことができます。

以上がPyTorchでモデルを定義し学習させる一般的な手順です。次項では、GPUを使って計算を高速化する方法について触れます。

### GPUの活用と並列計算
深層学習の計算は行列演算が主体であり、画像や動画、3Dデータなどでは計算量が莫大になります。そのため、**GPU（グラフィックス処理装置）**を用いた並列計算の活用は不可欠です。PyTorchでは、CUDA対応のNVIDIA GPUが利用可能であれば簡単に演算をGPUにオフロードすることができます。

基本は、**テンソル**や**モデル**をGPU上に載せてしまうことです。PyTorchのテンソルは `device` という属性を持っており、生成時に `device=torch.device("cuda")` などと指定するか、既存テンソルに対して `.to(device)` を呼ぶことでGPUへ移せます。同様に、`nn.Module` 継承のモデルも `model.to(device)` で内部の全パラメータをGPUに移せます。一度GPUに乗ったテンソル同士の演算は自動的にGPU上で実行され、大幅な高速化が得られます。

具体的な使用例：

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
for inputs, labels in train_loader:
    inputs = inputs.to(device)
    labels = labels.to(device)
    outputs = model(inputs)
    ...
```

上記のように、データとモデルを `device`（CUDAが利用可能ならGPU、なければCPU）に送ってから計算します。こうすることで、順伝播・逆伝播ともにGPU上で並列計算されます。特に畳み込みや行列積のような演算はGPUの得意分野であり、CPUに比べ桁違いの速度向上が得られます。

また、複数GPUがある場合は**データ並列**も容易に実現できます。PyTorchでは `nn.DataParallel` や `DistributedDataParallel` を使ってモデルを複数GPU上で並列に動作させることが可能です。`DataParallel` を用いれば、ミニバッチを自動的にGPU台数分に分割し、各GPUで並行して順伝播・逆伝播を行った後に勾配を集約して更新する、という処理をしてくれます。コード上はモデルを `model = nn.DataParallel(model)` でラップするだけで機能します（ただし最新では分散データ並列 `DistributedDataParallel` の使用が推奨されています）。

GPUを用いる際は、そのメモリ（VRAM）容量に注意が必要です。特に画像や映像、巨大なモデルではVRAM消費が激しく、バッチサイズやモデルサイズを調整しないとメモリ不足になることがあります。PyTorchではCUDAのメモリ割り当てエラーが出た場合、スタックトレースを読んで容量を超えていないか確認します。また、不要になったテンソルは `del` や `with torch.no_grad():` を使ってメモリを解放する、あるいは適宜 `torch.cuda.empty_cache()` を呼ぶなどして管理します。

このように、PyTorchでは簡単なコード修正でGPUアクセラレーションが可能であり、ディープラーニングの実験を高速に行えます。並列計算の恩恵を最大限に活かすことで、複雑なモデルの学習も現実的な時間で完了させることができます。

### モデルの保存と読み込み
学習済みモデルを保存し、後で再利用したり他の環境で推論に使ったりすることも深層学習の実践には重要です。PyTorchでは`torch.save` 関数と `torch.load` 関数を使ってモデルのパラメータを簡単に保存・復元できます。

一般的には、モデルの**状態辞書（state_dict）**を保存する方法が推奨されています。状態辞書とは、モデル内部の全てのパラメータ（重みやバイアス、バッチノルムの統計量など）のテンソルを保持したPythonの辞書オブジェクトです。たとえば上で定義した `model` を保存するには次のようにします。

```python
# モデルの保存
torch.save(model.state_dict(), "mymodel_weights.pth")
```

これによりファイル `"mymodel_weights.pth"` にモデルのパラメータがシリアライズされて保存されます。一般的に拡張子は `.pth` や `.pt` が用いられます。学習再開（Checkpoint）用にエポック数やOptimizerの状態も含めて辞書にまとめて保存することもありますが、ここでは単純に重みのみ保存しました。

保存したモデルをロードするには、まず同じモデル構造のインスタンスを用意してから、その `state_dict` に保存済みパラメータを読み込みます。

```python
model = MyNet()  # 同じネットワーク構造のモデルを作成
model.load_state_dict(torch.load("mymodel_weights.pth"))
model.eval()     # 推論モードに切り替え（Dropout等無効化）
```

これで `model` が学習時と同じパラメータを持つ状態になります。あとは `model(test_input)` で推論に使えます。`eval()` を呼ぶのは推論時の振る舞いにするためで、学習を再開する場合は `model.train()` のままで構いません。

なお、`torch.save(model, "model.pth")` とモデルオブジェクトごと直列化することもできますが、これは後でコード上で同じクラス定義がないとロードできず、また互換性も下位バージョンでは保証されないため、**推奨は state_dict の保存**です。同様にオプティマイザの状態も `optimizer.state_dict()` で保存・復元できます。

以上により、長時間学習するモデルを途中で保存したり、学習済みモデルを配布して別のデータで微調整（ファインチューニング）したりといったことが可能になります。

### 実践演習：画像分類モデルの構築
それでは、PyTorchを使った**画像分類モデルの構築**を実践してみましょう。ここでは、カラー画像を10クラスに分類する古典的なデータセット「CIFAR-10」を題材に、簡単な畳み込みニューラルネットワーク（CNN）のモデルを一から実装し、学習させます。CNNは次章で詳述しますが、ここでは「画像に畳み込み層とプーリング層を適用して特徴を抽出し、最後に全結合層で分類するネットワーク」という程度の理解で進めます。

**データ準備**：CIFAR-10は32×32ピクセル、3チャンネル（カラー）の画像からなり、10種の物体カテゴリ（飛行機、自動車、鳥、猫、鹿、犬、カエル、馬、船、トラック）に分類されます。PyTorchの `torchvision.datasets` からダウンロード可能です。

```python
import torchvision.datasets as dsets
import torchvision.transforms as T

# CIFAR-10データセットの読み込み
transform = T.Compose([
    T.ToTensor(),                      # テンソルに変換（[0,1]に正規化）
    T.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))  # 平均0.5, 標準偏差0.5で正規化
])
train_set = dsets.CIFAR10(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)
```

**モデル構築**：次にCNNモデルを定義します。基本構造は、「畳み込み層→ReLU→プーリング層」を2回繰り返し、その後出力を全結合層に入力してクラス分類する形にします。具体的には、最初の畳み込み層で入力画像(3ch)から16chの特徴マップを作り、2×2プーリングでダウンサンプル。次の畳み込み層で32chに増やし、再度プーリング。最後に32chの特徴マップをベクトルに_flatten_してから、全結合層で10クラスのスコアを出力します。

```python
class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=5)   # 畳み込み層1: 入力3ch, 出力16ch, 5x5フィルタ
        self.conv2 = nn.Conv2d(16, 32, kernel_size=5)  # 畳み込み層2: 入力16ch, 出力32ch, 5x5フィルタ
        self.pool = nn.MaxPool2d(2, 2)                 # プーリング層: 2x2のMax Pool
        self.fc1 = nn.Linear(32 * 5 * 5, 64)           # 全結合層1: 畳み込み出力を受けて64ユニット
        self.fc2 = nn.Linear(64, 10)                   # 全結合層2: 出力層（10クラス）
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = self.pool(x)                    # 32x32 -> 16x16 にプーリングで縮小
        x = torch.relu(self.conv2(x))
        x = self.pool(x)                    # 16x16 -> 8x8 に縮小
        x = x.view(x.size(0), -1)           # ベクトルに展開（32*5*5=800次元）
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x
```

このモデル `SimpleCNN` は、典型的な小規模CNN（LeNet風）の構造です。では、このモデルをCIFAR-10の訓練データで学習させてみましょう。

```python
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adamオプティマイザを使用

# 学習ループ（簡略版）
for epoch in range(5):
    running_loss = 0.0
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch+1}: Loss = {running_loss/len(train_loader):.4f}")
```

上記は5エポック学習するループです（実際には精度向上のためもっと多く回します）。ループ内では順伝播・損失計算・逆伝播・重み更新を行っています。学習が進むとLoss（損失）が徐々に下がっていくのが確認できるでしょう。最終的にこの簡単なCNNでも、訓練データに対してかなりの精度で10クラス分類を行えるようになります（検証データでの精度はおおよそ50～70%程度になりますが、これはモデルが小さいためで、より大きなネットワークやデータ拡張を使えば精度向上が可能です）。

以上、PyTorchを用いた画像分類モデル構築の一連の流れを示しました。データ準備、モデル定義、学習ループ、評価という手順はどんなタスクでも共通です。PyTorchの柔軟な構造を活用して、様々なモデルを実装・訓練できるようになることが深層学習実践への第一歩です。

## 第6章：畳み込みニューラルネットワーク

### 画像データの扱い方
これまで扱ってきた手書き数字やCIFAR-10の例のように、**画像データ**は高さ・幅・（場合によってはチャンネル）からなる多次元配列です。たとえばカラー画像は「縦×横×3チャネル」のピクセル値（RGB）の集まりとして表現できます。ニューラルネットワークで画像を扱う場合、基本的な方法は**画像の各ピクセルを入力とする**ことですが、単純にピクセルを1次元に並べて全結合層に入力すると、空間的な隣接関係などの情報が失われ、膨大な数のパラメータが必要になります。そこで登場したのが**畳み込みニューラルネットワーク (Convolutional Neural Network, CNN)**で、これは画像などの格子状データに特化したアーキテクチャです。

CNNでは画像データをそのまま2次元（+チャネル）構造として入力し、**畳み込み層**で局所的なパターンを検出し、**プーリング層**で空間解像度を下げながら特徴の要約を行います。すなわち、**画像の一部領域から特徴量を抽出し、少し位置がずれても同じ特徴として認識できるようにする**処理が組み込まれています ([畳み込みニューラルネットワークの基礎を理解する | リーディング・エッジ社　旧・研究開発部ブログ](https://leadinge.co.jp/rd/2021/06/07/863/#:~:text=%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A8%E3%81%AF%E3%80%81%E7%94%BB%E5%83%8F%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E5%85%A5%E5%8A%9B%E3%81%A8%E3%81%97%E3%81%A6%E3%80%81%E9%AB%98%E3%81%84%E8%AA%8D%E8%AD%98%E6%80%A7%E8%83%BD%E3%82%92%E9%81%94%E6%88%90%E3%81%A7%E3%81%8D%E3%82%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%99%E3%80%82%E9%80%9A%E5%B8%B8%E3%81%AE%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%A8%E5%90%8C%E6%A7%98%E3%81%AB%E3%80%81%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95%E3%82%92%E5%AD%A6%E7%BF%92%E3%81%AB%E4%BD%BF%E3%81%84%E3%81%BE%20%E3%81%99%E3%80%82%E4%B8%8B%E8%A8%98%E3%81%AE%E3%82%88%E3%81%86%E3%81%AA%E3%80%81%E7%A7%81%E3%81%9F%E3%81%A1%E4%BA%BA%E9%96%93%E3%81%8C%E6%8C%81%E3%81%A3%E3%81%A6%E3%81%84%E3%82%8B%E8%A6%96%E8%A6%9A%E9%87%8E%E3%81%AE%E7%A5%9E%E7%B5%8C%E7%B4%B0%E8%83%9E%E3%81%AE%E5%83%8D%E3%81%8D%E3%82%92%E6%A8%A1%E5%80%A3%E3%81%97%E3%81%A6%E3%81%BF%E3%82%88%E3%81%86%E3%80%81%E3%81%A8%E3%81%84%E3%81%86%E7%99%BA%E6%83%B3%E3%81%8B%E3%82%89%E7%94%9F%E3%81%BE%E3%82%8C%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82)) ([畳み込みニューラルネットワークの基礎を理解する | リーディング・エッジ社　旧・研究開発部ブログ](https://leadinge.co.jp/rd/2021/06/07/863/#:~:text=vol.%20J62,665))。人間の視覚野における単純型細胞（エッジ検出など）と複雑型細胞（位置ずれに不変な特徴検出）の仕組みにならったもので、画像認識に高い性能を発揮します ([畳み込みニューラルネットワークの基礎を理解する | リーディング・エッジ社　旧・研究開発部ブログ](https://leadinge.co.jp/rd/2021/06/07/863/#:~:text=))。

具体的に、次節で述べる**畳み込み演算**により画像からエッジや模様などの**特徴マップ**を抽出し、プーリングで位置の多少のずれを吸収しつつ**データ量を圧縮**します。これらを繰り返すことで画像の高次の特徴（例えば「目」「口」といったパーツや、「顔」「車」といった構造）を段階的に捉えることができます ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=%E3%81%AF%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E6%A4%9C%E5%87%BA%E5%99%A8%E3%82%92%E8%A4%87%E6%95%B0%E6%8C%81%E3%81%A3%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%20%E6%9C%80%E5%88%9D%E3%81%AE%E5%B1%A4%E3%81%A7%E3%81%AF%E3%82%A8%E3%83%83%E3%82%B8%E3%81%AA%E3%81%A9%E4%BD%8E%E3%83%AC%E3%83%99%E3%83%AB%E3%81%AA%E6%83%85%E5%A0%B1%E3%82%92%E6%A4%9C%E5%87%BA%E3%81%97%E3%80%81%E5%B1%A4%E3%81%8C%E6%B7%B1%E3%81%8F%E3%81%AA%E3%82%8B%E3%81%AB%E3%81%97%E3%81%9F%E3%81%8C%E3%81%A3%E3%81%A6%E3%82%88%E3%82%8A%E6%8A%BD%E8%B1%A1%E7%9A%84%E3%81%AA%E7%89%B9%E5%BE%B4%E3%82%92%E6%A4%9C%E5%87%BA%20%E3%81%97%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82CNN%E3%81%AF%E3%81%93%E3%81%86%E3%81%84%E3%81%A3%E3%81%9F%E7%89%B9%E5%BE%B4%E3%82%92%E6%8A%BD%E5%87%BA%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E6%A4%9C%E5%87%BA%E5%99%A8%E3%81%A7%E3%81%82%E3%82%8B%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%92%E8%87%AA%E5%8B%95%E3%81%A7%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82%20%E6%9C%80%E7%B5%82%E7%9A%84%E3%81%AB%E3%81%AF%E5%87%BA%E5%8A%9B%E3%81%AB%E8%BF%91%E3%81%84%E5%B1%A4%E3%81%A7%E3%81%AF%E3%80%81%E5%85%A8%E7%B5%90%E5%90%88%E3%81%AB%E3%82%88%E3%82%8A%E5%88%86%E9%A1%9E%E3%81%AA%E3%81%A9%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%EF%BC%88%E4%B8%8A%E8%A8%98%E3%81%AE%E4%BE%8B%E3%81%A7%E3%81%AF%E3%80%81%E6%95%B0%E5%AD%97%EF%BC%90%EF%BD%9E%EF%BC%99%E3%81%AE%EF%BC%91%EF%BC%90%E5%88%86%E9%A1%9E%E3%81%AE%E3%82%B9%E3%82%B3%E3%82%A2%E3%83%BB%E7%A2%BA%E7%8E%87%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%97%E3%81%BE%E3%81%99%EF%BC%89%E3%80%82))。最終的に得られた特徴量を全結合層に渡してクラスに分類する、というのがCNNによる画像認識の流れです。

実装上、画像データはPyTorchでは形状が `(N, C, H, W)` （N: バッチサイズ, C: チャンネル数, H: 高さ, W: 幅）の4次元テンソルとして表現されます。例えば28×28の白黒画像なら (N,1,28,28)、32×32カラー画像なら (N,3,32,32) です。畳み込み層 (`nn.Conv2d`) はこのようなテンソルを受け取り、内部でフィルタ（カーネル）を適用して出力の特徴マップを計算します。では、その畳み込み演算の仕組みを詳しく見てみましょう。

### 畳み込み演算の仕組み
**畳み込み演算（Convolution)**は画像にフィルタ（カーネル）を適用して特徴を抽出する基本的な演算です。1つの**畳み込み層**は複数のフィルタを持ち、入力画像にそれらを走らせる（畳み込む）ことで複数の**特徴マップ**を出力します。畳み込み層の最初の段階ではエッジなどの低レベルな特徴を検出し、層が深くなるにつれてより抽象的な特徴を検出していきます。CNNはこの特徴検出器であるフィルタのパラメータを自動で学習します ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=%E3%81%AF%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E6%A4%9C%E5%87%BA%E5%99%A8%E3%82%92%E8%A4%87%E6%95%B0%E6%8C%81%E3%81%A3%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%20%E6%9C%80%E5%88%9D%E3%81%AE%E5%B1%A4%E3%81%A7%E3%81%AF%E3%82%A8%E3%83%83%E3%82%B8%E3%81%AA%E3%81%A9%E4%BD%8E%E3%83%AC%E3%83%99%E3%83%AB%E3%81%AA%E6%83%85%E5%A0%B1%E3%82%92%E6%A4%9C%E5%87%BA%E3%81%97%E3%80%81%E5%B1%A4%E3%81%8C%E6%B7%B1%E3%81%8F%E3%81%AA%E3%82%8B%E3%81%AB%E3%81%97%E3%81%9F%E3%81%8C%E3%81%A3%E3%81%A6%E3%82%88%E3%82%8A%E6%8A%BD%E8%B1%A1%E7%9A%84%E3%81%AA%E7%89%B9%E5%BE%B4%E3%82%92%E6%A4%9C%E5%87%BA%20%E3%81%97%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82CNN%E3%81%AF%E3%81%93%E3%81%86%E3%81%84%E3%81%A3%E3%81%9F%E7%89%B9%E5%BE%B4%E3%82%92%E6%8A%BD%E5%87%BA%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E6%A4%9C%E5%87%BA%E5%99%A8%E3%81%A7%E3%81%82%E3%82%8B%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%92%E8%87%AA%E5%8B%95%E3%81%A7%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82%20%E6%9C%80%E7%B5%82%E7%9A%84%E3%81%AB%E3%81%AF%E5%87%BA%E5%8A%9B%E3%81%AB%E8%BF%91%E3%81%84%E5%B1%A4%E3%81%A7%E3%81%AF%E3%80%81%E5%85%A8%E7%B5%90%E5%90%88%E3%81%AB%E3%82%88%E3%82%8A%E5%88%86%E9%A1%9E%E3%81%AA%E3%81%A9%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%EF%BC%88%E4%B8%8A%E8%A8%98%E3%81%AE%E4%BE%8B%E3%81%A7%E3%81%AF%E3%80%81%E6%95%B0%E5%AD%97%EF%BC%90%EF%BD%9E%EF%BC%99%E3%81%AE%EF%BC%91%EF%BC%90%E5%88%86%E9%A1%9E%E3%81%AE%E3%82%B9%E3%82%B3%E3%82%A2%E3%83%BB%E7%A2%BA%E7%8E%87%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%97%E3%81%BE%E3%81%99%EF%BC%89%E3%80%82))。最終的に出力に近い層では全結合層によって分類が行われます（例えば数字認識なら0～9のスコアを出力します） ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=%E3%81%AF%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E6%A4%9C%E5%87%BA%E5%99%A8%E3%82%92%E8%A4%87%E6%95%B0%E6%8C%81%E3%81%A3%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%20%E6%9C%80%E5%88%9D%E3%81%AE%E5%B1%A4%E3%81%A7%E3%81%AF%E3%82%A8%E3%83%83%E3%82%B8%E3%81%AA%E3%81%A9%E4%BD%8E%E3%83%AC%E3%83%99%E3%83%AB%E3%81%AA%E6%83%85%E5%A0%B1%E3%82%92%E6%A4%9C%E5%87%BA%E3%81%97%E3%80%81%E5%B1%A4%E3%81%8C%E6%B7%B1%E3%81%8F%E3%81%AA%E3%82%8B%E3%81%AB%E3%81%97%E3%81%9F%E3%81%8C%E3%81%A3%E3%81%A6%E3%82%88%E3%82%8A%E6%8A%BD%E8%B1%A1%E7%9A%84%E3%81%AA%E7%89%B9%E5%BE%B4%E3%82%92%E6%A4%9C%E5%87%BA%20%E3%81%97%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82CNN%E3%81%AF%E3%81%93%E3%81%86%E3%81%84%E3%81%A3%E3%81%9F%E7%89%B9%E5%BE%B4%E3%82%92%E6%8A%BD%E5%87%BA%E3%81%99%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AE%E6%A4%9C%E5%87%BA%E5%99%A8%E3%81%A7%E3%81%82%E3%82%8B%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%92%E8%87%AA%E5%8B%95%E3%81%A7%E5%AD%A6%E7%BF%92%E3%81%97%E3%81%A6%E3%81%84%E3%81%8D%E3%81%BE%E3%81%99%E3%80%82%20%E6%9C%80%E7%B5%82%E7%9A%84%E3%81%AB%E3%81%AF%E5%87%BA%E5%8A%9B%E3%81%AB%E8%BF%91%E3%81%84%E5%B1%A4%E3%81%A7%E3%81%AF%E3%80%81%E5%85%A8%E7%B5%90%E5%90%88%E3%81%AB%E3%82%88%E3%82%8A%E5%88%86%E9%A1%9E%E3%81%AA%E3%81%A9%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%EF%BC%88%E4%B8%8A%E8%A8%98%E3%81%AE%E4%BE%8B%E3%81%A7%E3%81%AF%E3%80%81%E6%95%B0%E5%AD%97%EF%BC%90%EF%BD%9E%EF%BC%99%E3%81%AE%EF%BC%91%EF%BC%90%E5%88%86%E9%A1%9E%E3%81%AE%E3%82%B9%E3%82%B3%E3%82%A2%E3%83%BB%E7%A2%BA%E7%8E%87%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%97%E3%81%BE%E3%81%99%EF%BC%89%E3%80%82))。

畳み込み演算では、フィルタと入力画像の**局所領域**との積和演算が行われます ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=match%20at%20L113%20%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E6%BC%94%E7%AE%97%E3%81%AF%E3%80%81%E4%B8%8B%E5%9B%B3%E3%81%AB%E7%A4%BA%E3%81%99%E3%82%88%E3%81%86%E3%81%AB%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%82%92%E9%81%A9%E7%94%A8%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%82%92%E4%B8%80%E5%AE%9A%E3%81%AE%E9%96%93%E9%9A%94%EF%BC%88%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89%EF%BC%89%E3%81%A7%E3%82%B9%20%E3%83%A9%E3%82%A4%E3%83%89%E3%81%95%E3%81%9B%E3%81%AA%E3%81%8C%E3%82%89%E6%BC%94%E7%AE%97%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB%E3%81%AF%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E9%A0%86%E6%AC%A1%E9%87%8D%E3%81%AD%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E8%A6%81%E7%B4%A0%E3%81%A8%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E8%A6%81%E7%B4%A0%E3%82%92%E4%B9%97%E7%AE%97%E3%81%97%E3%81%9D%E3%82%8C%E3%81%AE%E5%92%8C%E3%82%92%E6%B1%82%E3%82%81%E3%81%A6%EF%BC%88%E7%A9%8D%E5%92%8C%E6%BC%94%E7%AE%97%E3%81%A8%E3%81%84,%E3%81%86%EF%BC%89%E3%80%81%E5%87%BA%E5%8A%9B%E7%B5%90%E6%9E%9C%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E5%A0%B4%E6%89%80%EF%BC%88%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E9%A0%86%EF%BC%89%E3%81%AB%E6%A0%BC%E7%B4%8D%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。2次元の場合を説明します。例えばサイズ \(H\times W\) の入力画像に対し、サイズ \(k\times k\) のフィルタ（カーネル）を適用するとします。フィルタは小さな行列で、例えば3×3や5×5のサイズです。このフィルタを画像の左上から順に重ね合わせ、**対応する画素とフィルタ係数の積を全て足し合わせ**た値を出力画像（特徴マップ）の左上の画素値とします ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=match%20at%20L113%20%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E6%BC%94%E7%AE%97%E3%81%AF%E3%80%81%E4%B8%8B%E5%9B%B3%E3%81%AB%E7%A4%BA%E3%81%99%E3%82%88%E3%81%86%E3%81%AB%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%82%92%E9%81%A9%E7%94%A8%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%82%92%E4%B8%80%E5%AE%9A%E3%81%AE%E9%96%93%E9%9A%94%EF%BC%88%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89%EF%BC%89%E3%81%A7%E3%82%B9%20%E3%83%A9%E3%82%A4%E3%83%89%E3%81%95%E3%81%9B%E3%81%AA%E3%81%8C%E3%82%89%E6%BC%94%E7%AE%97%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB%E3%81%AF%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E9%A0%86%E6%AC%A1%E9%87%8D%E3%81%AD%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E8%A6%81%E7%B4%A0%E3%81%A8%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E8%A6%81%E7%B4%A0%E3%82%92%E4%B9%97%E7%AE%97%E3%81%97%E3%81%9D%E3%82%8C%E3%81%AE%E5%92%8C%E3%82%92%E6%B1%82%E3%82%81%E3%81%A6%EF%BC%88%E7%A9%8D%E5%92%8C%E6%BC%94%E7%AE%97%E3%81%A8%E3%81%84,%E3%81%86%EF%BC%89%E3%80%81%E5%87%BA%E5%8A%9B%E7%B5%90%E6%9E%9C%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E5%A0%B4%E6%89%80%EF%BC%88%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E9%A0%86%EF%BC%89%E3%81%AB%E6%A0%BC%E7%B4%8D%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。次にフィルタを画像上で一定量横にスライド（これを**ストライド**といいます）させて同様の積和演算を行い、出力の次の画素値を得ます ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=match%20at%20L113%20%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E6%BC%94%E7%AE%97%E3%81%AF%E3%80%81%E4%B8%8B%E5%9B%B3%E3%81%AB%E7%A4%BA%E3%81%99%E3%82%88%E3%81%86%E3%81%AB%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%82%92%E9%81%A9%E7%94%A8%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%82%92%E4%B8%80%E5%AE%9A%E3%81%AE%E9%96%93%E9%9A%94%EF%BC%88%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89%EF%BC%89%E3%81%A7%E3%82%B9%20%E3%83%A9%E3%82%A4%E3%83%89%E3%81%95%E3%81%9B%E3%81%AA%E3%81%8C%E3%82%89%E6%BC%94%E7%AE%97%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB%E3%81%AF%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E9%A0%86%E6%AC%A1%E9%87%8D%E3%81%AD%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E8%A6%81%E7%B4%A0%E3%81%A8%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E8%A6%81%E7%B4%A0%E3%82%92%E4%B9%97%E7%AE%97%E3%81%97%E3%81%9D%E3%82%8C%E3%81%AE%E5%92%8C%E3%82%92%E6%B1%82%E3%82%81%E3%81%A6%EF%BC%88%E7%A9%8D%E5%92%8C%E6%BC%94%E7%AE%97%E3%81%A8%E3%81%84,%E3%81%86%EF%BC%89%E3%80%81%E5%87%BA%E5%8A%9B%E7%B5%90%E6%9E%9C%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E5%A0%B4%E6%89%80%EF%BC%88%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E9%A0%86%EF%BC%89%E3%81%AB%E6%A0%BC%E7%B4%8D%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))。これを画像全域で繰り返すことで出力の特徴マップが得られます。ストライドが1ならフィルタ適用位置を1画素ずつずらしていき、ストライド2なら2画素飛ばしで移動します。例えば5×5フィルタ、ストライド1、ゼロパディングなしのケースでは、入力サイズが32なら出力サイズは \( (32 - 5)/1 + 1 = 28\) となります ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=match%20at%20L139%20,%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89%E3%82%B5%E3%82%A4%E3%82%BA%20%EF%BD%9D%20%EF%BC%8B%20%EF%BC%91))（一般に**出力サイズ** \(= \frac{\text{入力サイズ} + 2\times \text{パディング} - \text{フィルタサイズ}}{\text{ストライド}}\ + 1\) ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=match%20at%20L139%20,%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89%E3%82%B5%E3%82%A4%E3%82%BA%20%EF%BD%9D%20%EF%BC%8B%20%EF%BC%91))）。縁まで畳み込みたい場合は、入力画像の周囲をゼロで埋める**パディング**を施します。例えばフィルタ半径分のパディングを入れれば出力サイズは入力と同じになります。

各フィルタは特定のパターンを検出する**カーネル**と考えられます。例えばあるフィルタが水平エッジ検出用に学習されれば、そのフィルタを畳み込んだ特徴マップは入力画像中の水平な境界が強く反応するマップになります。複数のフィルタを用意すれば、様々な特徴を並行して抽出できます。畳み込み層の出力チャンネル数はフィルタの個数に対応し、各チャンネルが異なる特徴マップを表します。例えば入力が1チャンネル（グレースケール）でフィルタ数6の畳み込み層なら、出力は6チャンネルの特徴マップになります。入力にチャンネル次元がある場合（カラー3chなど）は、フィルタも同じチャネル数の深さを持ち、全チャンネル分の重みで積和した結果を1つの出力値とします。そのため出力チャンネル毎に独立したフィルタセットを持ち、例えば入力3ch・出力16chの畳み込み層では \(3\times (\text{kernel_size})^2\) 個の重みを持つフィルタを16個持つことになります。

以上が畳み込み演算の基本です。数式で書けば、入力画像 \(x\) にフィルタ（カーネル） \(w\) を適用した出力特徴マップ \(y\) は、

\[ y[i,j] = \sum_{u=0}^{k-1}\sum_{v=0}^{k-1} x[i+u,\, j+v] \cdot w[u,v] + b \]

のように表されます（\(b\) はバイアス項）※。この計算を全ての位置 \((i,j)\) で行うわけです。実装上は、深層学習フレームワークが高度に最適化された行列演算やFFTを使って計算してくれます。

畳み込み層の重要なポイントは**パラメータ共有**と**局所受容野**です。フィルタは画像中の全ての位置で同じ重みを使っているため（パラメータ共有）、全結合層に比べ大幅にパラメータ数を削減できます。またフィルタが見る範囲（受容野）は決まったサイズなので、局所的なパターンしか捉えませんが、逆に言えば小さな特徴を発見するのに適しています。層を重ねることで、下位層の出力（局所特徴）を材料により広い範囲の特徴を上位層で学習することになるため、結果的に入力全体のパターンも捉えられます。

※畳み込みの厳密な定義では、フィルタを左右上下反転してから移動させる処理ですが、機械学習分野では反転なしの**相関**の計算であっても便宜上「畳み込み」と呼ぶことが多いです。

### プーリング層とその効果
**プーリング層（Pooling Layer）**は、畳み込み層で得られた特徴マップを**ダウンサンプリング（縮小）**する層です。プーリング層は学習するパラメータを持たず、単純に入力領域の要約を計算します。典型的には**最大プーリング (Max Pooling)** と**平均プーリング (Average Pooling)** の2種類があります。

- **最大値プーリング（Max Pooling）**：各局所領域内の**最大値**を取ります。例えば2×2のウィンドウでストライド2のMax Poolingなら、入力特徴マップ上の2×2ピクセルごとに最大値を計算し、それを出力マップの1ピクセルとします ([畳み込みニューラルネットワークの基礎を理解する | リーディング・エッジ社　旧・研究開発部ブログ](https://leadinge.co.jp/rd/2021/06/07/863/#:~:text=%E4%B8%8A%E5%9B%B3%E3%81%AFmax%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E5%87%A6%E7%90%86%E3%82%92%E8%A1%A8%E3%81%97%E3%81%9F%E3%82%82%E3%81%AE%E3%81%A7%E3%81%99%E3%80%822%C3%972%E3%81%94%E3%81%A8%E3%81%AB%E7%89%B9%E5%BE%B4%E3%83%9E%E3%83%83%E3%83%97%E3%81%AE%E6%9C%80%E5%A4%A7%E5%80%A4%E3%82%92%E6%8A%BD%E5%87%BA%E3%81%97%E3%80%81%E6%96%B0%E3%81%9F%E3%81%AB%E3%83%80%E3%82%A6%E3%83%B3%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0%E7%94%BB%E5%83%8F%E3%82%92%E5%8F%96%E5%BE%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82max%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AE%E4%BB%96%E3%81%AB%E3%82%82%20%E3%80%81avg%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E3%81%84%E3%81%86%E5%B9%B3%E5%9D%87%E5%80%A4%E3%82%92%E3%81%A8%E3%82%8B%E5%87%A6%E7%90%86%E3%82%82%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82))。こうすると画像サイズは縦横それぞれ1/2になります。最大プーリングは、その領域内で最も強く反応した特徴だけを残すので、多少位置がずれても検出できますし、**特徴の存在**に注目した縮約となります ([最大値プーリング(Max Pooling) - CVMLエキスパートガイド](https://cvml-expertguide.net/terms/dl/layers/pooling-layer/max-pooling/#:~:text=%E5%9B%B31%E3%81%AF%EF%BC%8C%E5%A4%A7%E3%81%8D%E3%81%95))。
- **平均プーリング（Average Pooling）**：各局所領域内の**平均値**を取ります。同じ2×2の例なら4ピクセルの平均を計算して出力します ([畳み込みニューラルネットワークの基礎を理解する | リーディング・エッジ社　旧・研究開発部ブログ](https://leadinge.co.jp/rd/2021/06/07/863/#:~:text=%E4%B8%8A%E5%9B%B3%E3%81%AFmax%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E5%87%A6%E7%90%86%E3%82%92%E8%A1%A8%E3%81%97%E3%81%9F%E3%82%82%E3%81%AE%E3%81%A7%E3%81%99%E3%80%822%C3%972%E3%81%94%E3%81%A8%E3%81%AB%E7%89%B9%E5%BE%B4%E3%83%9E%E3%83%83%E3%83%97%E3%81%AE%E6%9C%80%E5%A4%A7%E5%80%A4%E3%82%92%E6%8A%BD%E5%87%BA%E3%81%97%E3%80%81%E6%96%B0%E3%81%9F%E3%81%AB%E3%83%80%E3%82%A6%E3%83%B3%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0%E7%94%BB%E5%83%8F%E3%82%92%E5%8F%96%E5%BE%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82max%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AE%E4%BB%96%E3%81%AB%E3%82%82%20%E3%80%81avg%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E3%81%84%E3%81%86%E5%B9%B3%E5%9D%87%E5%80%A4%E3%82%92%E3%81%A8%E3%82%8B%E5%87%A6%E7%90%86%E3%82%82%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82))。平均プーリングは全体的な値の傾向を残しつつノイズを減らす効果がありますが、現在のCNNではMaxプーリングの方が一般的です（非線形性が強いため）。

プーリング層を挟むことで、特徴マップのサイズ（空間解像度）はどんどん小さくなります。例えば32×32→16×16→8×8と縮小すれば、各出力ピクセルは入力の大きな領域に対応する**受容野**を持つようになります。これによって高次の層ほど広い文脈を考慮した特徴を扱えるようになります。また、プーリングによる次元削減は計算量とパラメータ数を減らし、**過学習の抑制**にも役立ちます ([【0から学ぶAI】第77回：プーリング層 - PROMPT](https://service.ai-prompt.jp/article/ai365-077/#:~:text=0%E3%81%8B%E3%82%89%E5%AD%A6%E3%81%B6AI%20%E7%AC%AC77%E5%9B%9E%EF%BC%9A%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E5%B1%A4%20,)) ([CS 230 - 畳み込みニューラルネットワーク チートシート](https://stanford.edu/~shervine/l/ja/teaching/cs-230/cheatsheet-convolutional-neural-networks#:~:text=%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%20%28POOL%29%20%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E5%B1%A4%20%28POOL%29%E3%81%AF%E4%BD%8D%E7%BD%AE%E4%B8%8D%E5%A4%89%E6%80%A7%E3%82%92%E3%82%82%E3%81%A4%E7%B8%AE%E5%B0%8F%E6%93%8D%E4%BD%9C%E3%81%A7%E3%80%81%E9%80%9A%E5%B8%B8%E3%81%AF%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E5%B1%A4%E3%81%AE%E5%BE%8C%E3%81%AB%E9%81%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%E7%89%B9%E3%81%AB%E3%80%81%E6%9C%80%E5%A4%A7%E5%8F%8A%E3%81%B3%E5%B9%B3%E5%9D%87%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AF%E3%81%9D%E3%82%8C%E3%81%9E%E3%82%8C%E6%9C%80%E5%A4%A7%E3%81%A8%E5%B9%B3%E5%9D%87%E5%80%A4%E3%81%8C%20))。さらに、最大値プーリングであれば多少入力が平行移動しても最大値は大きく変わらないため、**位置に対する不変性（ロバストさ）**が獲得できます ([CS 230 - 畳み込みニューラルネットワーク チートシート](https://stanford.edu/~shervine/l/ja/teaching/cs-230/cheatsheet-convolutional-neural-networks#:~:text=%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%20%28POOL%29%20%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E5%B1%A4%20%28POOL%29%E3%81%AF%E4%BD%8D%E7%BD%AE%E4%B8%8D%E5%A4%89%E6%80%A7%E3%82%92%E3%82%82%E3%81%A4%E7%B8%AE%E5%B0%8F%E6%93%8D%E4%BD%9C%E3%81%A7%E3%80%81%E9%80%9A%E5%B8%B8%E3%81%AF%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E5%B1%A4%E3%81%AE%E5%BE%8C%E3%81%AB%E9%81%A9%E7%94%A8%E3%81%95%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82%E7%89%B9%E3%81%AB%E3%80%81%E6%9C%80%E5%A4%A7%E5%8F%8A%E3%81%B3%E5%B9%B3%E5%9D%87%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AF%E3%81%9D%E3%82%8C%E3%81%9E%E3%82%8C%E6%9C%80%E5%A4%A7%E3%81%A8%E5%B9%B3%E5%9D%87%E5%80%A4%E3%81%8C%20))。

具体的な効果をまとめると、プーリング層は**「データを縮小して重要な情報を凝縮し、計算効率を上げながら位置ずれに強い特徴表現を作る」**役割を果たします ([プーリング - zero to one](https://zero2one.jp/ai-word/pooling/?srsltid=AfmBOooABRflFe4YVnx8twmS5WzJ2sgptsrSV6OAZXE1PmhRH5UQCsM8#:~:text=%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%AF%E7%94%BB%E5%83%8F%E3%82%B5%E3%82%A4%E3%82%BA%E3%82%92%E8%A6%8F%E5%89%87%E3%81%AB%E5%BE%93%E3%81%A3%E3%81%A6%E7%B8%AE%E5%B0%8F%E3%81%99%E3%82%8B%E5%87%A6%E7%90%86%E3%81%A7%E3%81%99%E3%80%82%20%E7%94%BB%E5%83%8F%E3%81%AE%E5%A0%B4%E5%90%88%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E3%83%97%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E5%87%A6%E7%90%86%E3%82%92%E8%A1%8C%E3%81%AA%E3%81%A3%E3%81%A6%E3%81%8A%E3%82%8A%E3%80%812%C3%972%E3%81%AE%E5%B0%8F%E9%A0%98%E5%9F%9F%28%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%82%B5%E3%82%A4%E3%82%BA%29%E3%81%AE%E6%9C%80%E5%A4%A7%E5%80%A4%E3%82%92%E5%87%BA%E5%8A%9B%E3%81%97%20))。現在の一般的なCNNでは、畳み込み層の後に2×2ストライド2のMaxプーリングを入れる構成が定番です。

### 代表的なCNNアーキテクチャ
CNNは1990年代から研究されていましたが、2012年の画像認識コンペImageNet ILSVRCでの大規模CNNの成功以降、さまざまな**アーキテクチャ（モデル構造）**が提案され飛躍的に発展しました。ここでは歴史的に重要な代表的CNNアーキテクチャをいくつか紹介します ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E3%81%BE%E3%81%9A%E3%81%AF%E3%80%81ILSVRC2012%E3%81%A7%E5%84%AA%E5%8B%9D%E3%81%97%E3%81%9FAlexNet%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82%E3%81%93%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E3%80%81%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8C%E4%B8%80%E8%BA%8D%E8%84%9A%E5%85%89%E3%82%92%E6%B5%B4%E3%81%B3%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。

- **LeNet-5 (1998)** – Yann LeCunらによる手書き文字認識用のCNNです。入力32×32白黒画像に対し、畳み込み層と平均プーリング層を交互に重ね、最後に全結合層で10分類する7層ネットワークでした。活性化関数にシグモイド、学習にバックプロパゲーションを用い、手書き数字認識で高精度を達成しました。CNNの原型とも言えるモデルです。

- **AlexNet (2012)** – Alex Krizhevskyらによるモデルで、ImageNet大型画像データセット（1000クラス）で画期的な結果を出しました ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E3%81%BE%E3%81%9A%E3%81%AF%E3%80%81ILSVRC2012%E3%81%A7%E5%84%AA%E5%8B%9D%E3%81%97%E3%81%9FAlexNet%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82%E3%81%93%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E3%80%81%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8C%E4%B8%80%E8%BA%8D%E8%84%9A%E5%85%89%E3%82%92%E6%B5%B4%E3%81%B3%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。8層からなり（5つの畳み込み層＋3つの全結合層）、**ReLU活性化関数**の採用、**ドロップアウト**による正則化、大規模データに対応した**GPU並列計算**など当時最新の手法を盛り込みました。これにより従来の手法に比べ大幅な精度向上を果たし、ディープラーニングブームの火付け役となりました ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。

- **VGG (2014)** – オックスフォード大学のSimonyanらによるモデルで、非常にシンプルな構成ながら高性能を示しました。畳み込みフィルタをすべて3×3に統一し、その代わり層数を深くする（16層や19層）ことで表現力を上げています。例えばVGG-16は13個の畳み込み層と3個の全結合層からなります。シンプルな構造ゆえ解析もしやすく、以後のモデルの比較ベースラインとしても用いられました ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E3%81%BE%E3%81%9A%E3%81%AF%E3%80%81ILSVRC2012%E3%81%A7%E5%84%AA%E5%8B%9D%E3%81%97%E3%81%9FAlexNet%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82%E3%81%93%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E3%80%81%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8C%E4%B8%80%E8%BA%8D%E8%84%9A%E5%85%89%E3%82%92%E6%B5%B4%E3%81%B3%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。

- **GoogLeNet (Inception, 2014)** – Googleチームによるモデルで、**ILSVRC2014で優勝**した22層のCNNです ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%94%BB%E5%83%8F%E5%88%86%E9%A1%9E%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A33%EF%BC%9AGoogleNet)) ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E3%81%BE%E3%81%9A%E3%81%AF%E3%80%81ILSVRC2012%E3%81%A7%E5%84%AA%E5%8B%9D%E3%81%97%E3%81%9FAlexNet%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82%E3%81%93%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E3%80%81%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8C%E4%B8%80%E8%BA%8D%E8%84%9A%E5%85%89%E3%82%92%E6%B5%B4%E3%81%B3%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。特徴は「**Inceptionモジュール**」と呼ばれる複合畳み込みブロックを採用したことです。一つの層で異なるサイズの畳み込み（1×1,3×3,5×5）やプーリングを並列に行い、それらの出力を結合することで、様々なスケールの特徴を捉える工夫をしました。また1×1畳み込みでチャネル数を削減することで計算量の増加を抑えています ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=match%20at%20L292%20GoogLeNet%E3%81%A7%E3%81%AF%E3%80%81Inception%20module%E3%82%92%E5%A4%9A%E6%95%B0%E4%BD%BF%E3%81%86%E3%81%93%E3%81%A8%E3%81%A7%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%BC%E3%81%8C%E8%86%A8%E5%A4%A7%E3%81%AA%E6%95%B0%E3%81%AB%E3%81%AA%E3%82%8B%E3%81%9F%E3%82%81%E3%80%81%E5%90%84%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E8%A8%88%E7%AE%97%E3%81%AE%E5%89%8D%E3%81%AB,1%C3%971%20Convolution%20%E3%82%92%E8%A1%8C%E3%81%84%E3%80%81%E6%AC%A1%E5%85%83%E3%82%92%E5%89%8A%E6%B8%9B%E3%81%97%E3%81%A6%E3%81%84%E3%81%BE%E3%81%99%E3%80%82))。

- **ResNet (2015)** – Microsoft ResearchのKaiming Heらによるモデルで、**152層もの超深いネットワーク**でILSVRC2015を制した画期的アーキテクチャです ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82)) ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。ResNet最大の特徴は**残差ブロック（Residual Connection）**の導入です。これは層の出力に入力をそのまま足し合わせるスキップ接続で、これによって勾配が深い層まで伝わりやすくなり、**勾配消失**の問題を緩和して極端な深層でも学習を可能にしました ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。ResNet以降、「より深く」が可能になり、多くの派生モデル（ResNeXtやWideResNetなど）も生まれました。

- **DenseNet (2017)** – ResNetを発展させたものの一つで、各層で前のすべての層の出力を連結（Denseに接続）するというアーキテクチャです。特徴再利用を極限まで進め、パラメータ効率が高く高精度なモデルとなりました ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。

- **MobileNet (2017)** – モバイルや組み込み向けに開発された軽量CNNです。畳み込みをDepthwise（チャネル毎独立）とPointwise（1×1畳み込み）に分解する**Depthwise Separable Convolution**で計算量を削減するなどの工夫により、スマートフォンでもリアルタイムに動作可能な軽量さを実現しました ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%94%BB%E5%83%8F%E5%88%86%E9%A1%9E%E3%82%A2%E3%83%BC%E3%82%AD%E3%83%86%E3%82%AF%E3%83%81%E3%83%A36%EF%BC%9AMobileNet))。MobileNetシリーズは現在v3まで発展しています ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=match%20at%20L531%20%E6%9C%80%E5%BE%8C%E3%81%AEMobileNet%E3%81%AF%E3%80%81%E7%8F%BE%E5%9C%A8%E3%81%A7%E3%81%AFVersion3%E3%81%BE%E3%81%A7%E9%96%8B%E7%99%BA%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%83%A2%E3%83%90%E3%82%A4%E3%83%AB%E7%AB%AF%E6%9C%AB%E3%81%A7%E3%82%82%E4%BD%BF%E7%94%A8%E3%81%95%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82))。

これら以外にも、画像認識以外のタスク向けのCNN派生（セグメンテーション向けのU-NetやPSPNet、検出向けのYOLOやFaster R-CNNなど）も多数存在します。近年はVision TransformerのようにCNN以外の手法も台頭していますが、基本原理として畳み込みニューラルネットワークの考え方はしっかり押さえておく必要があります。

### 転移学習の活用
深層学習モデルは大量のデータで一から学習させるのが理想ですが、現実にはそれだけのデータが用意できないことも多々あります。そこで役立つのが**転移学習（Transfer Learning）**です。転移学習とは、**既に学習済みのモデル（大規模データで訓練されたモデル）の知識を別の関連する課題に転用する手法**です。

典型的なケースとして、Imagenetで学習済みのCNNを用いて、医療画像や小規模データセットでの分類タスクに応用する、といったことが行われます。具体的なやり方は2通りあります：

- **特徴抽出器として利用**：学習済みモデルの重みをそのまま使い、最終層（分類層）のみを自分のタスク用に置き換えて学習し直す方法です。例えばImagenet1000クラスで学習済みのResNet50を取り出し、最後の全結合層を自分のデータのクラス数（例えば5クラス）に付け替えます ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。そして元の畳み込み部分の重みは凍結（freeze）して変更しないようにし、最後の新しい層だけを訓練します。こうすると、すでに学習済みの低レベル～中レベルの特徴（エッジや模様、物体の形状など）を活用しつつ、自分のタスクに合わせた出力だけ学習できます。少ないデータでも高い性能が得られやすく、計算資源も節約できます。

- **ファインチューニング（微調整）**：学習済みモデル全体を初期値として、そこからさらに自分のデータで**再学習**させる方法です。多くの場合、まず上記のように分類層を付け替えて全体を少し学習させ（ウォームアップ）、その後一部あるいは全ての層の重みの凍結を解除して**低めの学習率**で追加学習します。これにより、新しいタスクに合わせて重みを微調整します。大量のデータはないがある程度はある場合や、元のタスクと新タスクの違いが大きい場合に有効です。

PyTorchでは、`torchvision.models` から様々な学習済みモデルを取得できます。例えばResNet18の学習済みモデルは以下のようにロードし、転移学習用に加工できます。

```python
from torchvision import models
resnet = models.resnet18(pretrained=True)    # ImageNetで訓練済みのResNet18をロード

# 特徴抽出モードにする場合：全パラメータを凍結
for param in resnet.parameters():
    param.requires_grad = False

# 出力層を置き換える（例えば新しい分類クラス数が5の場合）
num_features = resnet.fc.in_features  # ResNetの全結合層の入力次元
resnet.fc = nn.Linear(num_features, 5)
```

上記で、`resnet` の畳み込み層部分は学習済みの重みを保持しつつフリーズされ、最後の全結合層（`resnet.fc`）だけがランダム初期化された5クラス出力の層に差し替わりました。あとはこのモデルに対して自分の5クラスデータで通常通り学習を行えば、転移学習が可能です。初めはfc層のみ学習し、必要に応じて一部畳み込み層も `requires_grad=True` にして微調整します。

転移学習の効果は絶大で、ゼロから学習するのに比べてはるかに短時間で高精度に到達することが多いです。特に画像認識ではImageNetと対象領域が近い場合、少数のデータでも高い汎化性能が得られます ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))。これは畳み込み層が学習するエッジや質感といった特徴は様々な視覚タスクで共通に有用だからです。

実際の活用例として、医療分野でX線画像から疾患分類をする際にImageNetで学習済みのResNetを転移学習したり、動物や植物の画像分類コンペで学習済みモデルを微調整したり、といったことが行われています。転移学習はディープラーニング実践において強力なテクニックであり、データ不足を補い、計算コストを削減する手段として広く用いられています。

### 実践演習：物体検出アプリケーション
最後に、応用例として**物体検出（Object Detection）**タスクに挑戦してみましょう。物体検出は、画像中に「何が」「どこに」写っているかを特定する手法です ([〖YOLO〗の仕組みを簡単にまとめてみた〖物体検出アルゴリズム〗 #初心者 - Qiita](https://qiita.com/kindamu24005/items/efd53c7511a40ddac636#:~:text=%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA))。すなわち、画像内の対象物ごとに**クラス**（カテゴリ）と**位置（バウンディングボックス）**を出力します。これは単一のクラスを判別する画像分類よりも難易度が高く、ニューラルネットワークモデルもより複雑になります。

有名な物体検出アルゴリズムには、**R-CNN系列（Fast R-CNN, Faster R-CNNなど）**と**YOLO（You Only Look Once）系列**があります。前者は領域候補を生成してから分類する2段階方式、後者は画像をグリッドに分割して一度にバウンディングボックスとクラスを回帰する1段階方式です ([〖YOLO〗の仕組みを簡単にまとめてみた〖物体検出アルゴリズム〗 #初心者 - Qiita](https://qiita.com/kindamu24005/items/efd53c7511a40ddac636#:~:text=1))。現在の最新手法ではTransformerを使ったもの（DETRなど）も登場していますが、ここでは古典的なFaster R-CNNを使ってみます。

PyTorchの`torchvision.models`には物体検出用にFaster R-CNNやRetinaNetなどのプリトレイン済みモデルが用意されています。ここでは **Faster R-CNN + ResNet-50 FPN** のCOCOデータセット（80カテゴリ）で学習済みモデルを使って、入力画像から物体検出を行ってみます。

まずモデルをロードします（初回は自動的にインターネットから重みをダウンロードします）。

```python
import torchvision.transforms as T
from PIL import Image

# 学習済みのFaster R-CNNモデルをロード（80クラスのCOCOデータ）
model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model.eval()  # 推論モード

# 入力画像の読み込みと前処理
img = Image.open("test.jpg")  # 任意のテスト画像
transform = T.Compose([T.ToTensor()])  # PIL画像をテンソルに変換
input_tensor = transform(img)
```

次にモデルに入力を与えて推論します。物体検出モデルは画像1枚に対してもバッチとしてリストで渡す必要があるので、`[input_tensor]` のようにリストに包んで渡します。

```python
# 推論実行
outputs = model([input_tensor])
```

`outputs` には結果が格納されています。これはリストで、各画像に対する結果として辞書が含まれています。例えば単一画像の場合 `outputs[0]` が辞書で、キーは`boxes`（検出BBox座標）, `labels`（クラスID）, `scores`（信頼度）となっています。中身を少し見てみましょう。

```python
print(outputs[0].keys())
# dict_keys(['boxes', 'labels', 'scores'])

boxes = outputs[0]['boxes'].detach().numpy()
labels = outputs[0]['labels'].detach().numpy()
scores = outputs[0]['scores'].detach().numpy()

# スコアが高い検出のみ表示
for box, label, score in zip(boxes, labels, scores):
    if score > 0.6:  # 信頼度0.6以上のものだけ
        print(f"Class {label}: Score {score:.2f}, Box {box}")
```

COCOデータセットではラベル番号とクラス名の対応が定義されています（例えば1=person、人、3=car、自動車など）。高スコアのものを見れば、モデルが画像中の何を検出したか分かるでしょう。

例えばテスト画像が街中の写真であれば、出力として「Class 1: Score 0.98, Box [x1,y1,x2,y2]」のように**人 (person)**が検出され、同様に車や信号といった物体が一覧表示されます。これを画像に重ね描画すると視覚的に検出結果を確認できます。

 ([image]())実際の物体検出の例として、左図は1人の少女（person）と1匹の犬（dog）が検出された例、右図は編隊飛行する複数の飛行機（airplane）が検出された例です。それぞれの対象物に対し、適切なクラスラベルとバウンディングボックスが出力されていることが分かります。

物体検出では、このように**画像内の複数オブジェクトをローカライズし分類する**ため、分類だけのモデルに比べて出力も複雑です。学習には各物体に矩形の教師データが必要で、損失関数も分類誤差＋BBox回帰誤差の複合になります。Faster R-CNNのような2段階モデルではまず**RPN（Region Proposal Network）**が物体らしき領域候補を提案し、その後各領域を分類・微調整するという流れになります。一方YOLO系は回帰問題として統一的に扱うので非常に高速です ([〖YOLO〗の仕組みを簡単にまとめてみた〖物体検出アルゴリズム〗 #初心者 - Qiita](https://qiita.com/kindamu24005/items/efd53c7511a40ddac636#:~:text=1)) ([〖YOLO〗の仕組みを簡単にまとめてみた〖物体検出アルゴリズム〗 #初心者 - Qiita](https://qiita.com/kindamu24005/items/efd53c7511a40ddac636#:~:text=2))。どちらの手法でもCNNが特徴抽出に用いられており、分類ヘッドや回帰ヘッドを組み合わせて学習します。

今回はプリトレインモデルを使いましたが、自分で物体検出モデルを一から学習させることも可能です。PyTorchのTorchVision提供の実装を利用して転移学習することもできますし、Ultralytics社の`yolov5`パッケージなどを使えば比較的簡単に独自データでYOLOモデルを訓練できます。

物体検出は自動運転や監視システム、画像検索など応用範囲が広く、CNNの発展によって精度が大きく向上しました。分類より難しい分野ですが、基礎となる考え方は本章で説明したCNNの延長です。モデルの入出力が多少複雑になるだけで、活性化関数や畳み込み・プーリング等の原理は同じです。

以上、第6章では画像データに特化したディープラーニングモデルである畳み込みニューラルネットワーク（CNN）の基礎から応用までを概観しました。画像認識の分野ではCNNが長らく主流であり、多くの優れたアーキテクチャが提案されてきました。今後も新たな手法が登場するでしょうが、ここで学んだ基礎知識はそれらを理解し応用する上で必ず役立つはずです。

**参考文献・ソース**：ニューラルネットワークと学習手法の解説 ([パーセプトロンの仕組みや用語について解説 ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/perceptron/#:~:text=%E3%83%91%E3%83%BC%E3%82%BB%E3%83%97%E3%83%88%E3%83%AD%E3%83%B3%E3%81%AF%E5%85%A5%E5%8A%9B%20x%201%2Cx%202%20%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6,1%2Bw%202%20x%202%20%E3%82%92%E8%A8%88%E7%AE%97%E3%81%97%E3%81%BE%E3%81%99%E3%80%82)) ([〖ディープラーニング入門（第4回）〗勾配降下法を学んでディープラーニングの学習について理解しよう #DeepLearning - Qiita](https://qiita.com/kwi0303/items/7bfd7180f80a52296e64#:~:text=1))、活性化関数 ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=ReLU%E9%96%A2%E6%95%B0)) ([活性化関数の役割と代表的な活性化関数～ステップ・シグモイド・ReLU～ ｜AVILEN](https://avilen.co.jp/personal/knowledge-article/activation_function/#:~:text=h))、CNNの構造と画像認識への応用 ([畳み込みニューラルネットワークの基礎を理解する | リーディング・エッジ社　旧・研究開発部ブログ](https://leadinge.co.jp/rd/2021/06/07/863/#:~:text=vol.%20J62,665)) ([畳み込みネットワークCNN（Convolutional neural network） #Python - Qiita](https://qiita.com/DeepTama/items/379cac9a73c2aed7a082#:~:text=match%20at%20L113%20%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E6%BC%94%E7%AE%97%E3%81%AF%E3%80%81%E4%B8%8B%E5%9B%B3%E3%81%AB%E7%A4%BA%E3%81%99%E3%82%88%E3%81%86%E3%81%AB%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%EF%BC%88%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%81%A8%E3%82%82%E3%81%84%E3%81%86%EF%BC%89%E3%82%92%E9%81%A9%E7%94%A8%E3%81%97%E3%81%BE%E3%81%99%E3%80%82%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AB%E5%AF%BE%E3%81%97%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E3%82%A6%E3%82%A3%E3%83%B3%E3%83%89%E3%82%A6%E3%82%92%E4%B8%80%E5%AE%9A%E3%81%AE%E9%96%93%E9%9A%94%EF%BC%88%E3%82%B9%E3%83%88%E3%83%A9%E3%82%A4%E3%83%89%EF%BC%89%E3%81%A7%E3%82%B9%20%E3%83%A9%E3%82%A4%E3%83%89%E3%81%95%E3%81%9B%E3%81%AA%E3%81%8C%E3%82%89%E6%BC%94%E7%AE%97%E3%82%92%E8%A1%8C%E3%81%84%E3%81%BE%E3%81%99%E3%80%82%E5%85%B7%E4%BD%93%E7%9A%84%E3%81%AB%E3%81%AF%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E9%A0%86%E6%AC%A1%E9%87%8D%E3%81%AD%E3%81%A6%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%AE%E8%A6%81%E7%B4%A0%E3%81%A8%E5%85%A5%E5%8A%9B%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E8%A6%81%E7%B4%A0%E3%82%92%E4%B9%97%E7%AE%97%E3%81%97%E3%81%9D%E3%82%8C%E3%81%AE%E5%92%8C%E3%82%92%E6%B1%82%E3%82%81%E3%81%A6%EF%BC%88%E7%A9%8D%E5%92%8C%E6%BC%94%E7%AE%97%E3%81%A8%E3%81%84,%E3%81%86%EF%BC%89%E3%80%81%E5%87%BA%E5%8A%9B%E7%B5%90%E6%9E%9C%E3%81%AE%E5%AF%BE%E5%BF%9C%E3%81%99%E3%82%8B%E5%A0%B4%E6%89%80%EF%BC%88%E5%B7%A6%E4%B8%8A%E3%81%8B%E3%82%89%E9%A0%86%EF%BC%89%E3%81%AB%E6%A0%BC%E7%B4%8D%E3%81%97%E3%81%BE%E3%81%99%E3%80%82))、代表的アーキテクチャの特徴 ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E3%81%BE%E3%81%9A%E3%81%AF%E3%80%81ILSVRC2012%E3%81%A7%E5%84%AA%E5%8B%9D%E3%81%97%E3%81%9FAlexNet%E3%81%A7%E3%81%97%E3%81%9F%E3%80%82%E3%81%93%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%81%A3%E3%81%A6%E3%80%81%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%8C%E4%B8%80%E8%BA%8D%E8%84%9A%E5%85%89%E3%82%92%E6%B5%B4%E3%81%B3%E3%82%8B%E3%81%93%E3%81%A8%E3%81%A8%E3%81%AA%E3%82%8A%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82)) ([〖2025〗CNNと画像分類の6つの代表的なアーキテクチャの特徴まとめ | AI研究所](https://ai-kenkyujo.com/artificial-intelligence/ai-architecture-02/#:~:text=%E7%B6%9A%E3%81%84%E3%81%A6%E3%80%81%E3%81%93%E3%81%AEAlexNet%E3%82%88%E3%82%8A%E3%82%82%E5%B1%A4%E3%82%92%E6%B7%B1%E3%81%8F%E3%81%97%E3%81%9FVGG%E3%82%84GoogleNet%E3%81%8C%E7%99%BB%E5%A0%B4%E3%81%97%E3%80%81%E8%A8%98%E9%8C%B2%E3%82%92%E5%90%91%E4%B8%8A%E3%81%95%E3%81%9B%E3%81%A6%E3%81%8D%E3%81%BE%E3%81%97%E3%81%9F%E3%80%82))、物体検出の定義と例 ([〖YOLO〗の仕組みを簡単にまとめてみた〖物体検出アルゴリズム〗 #初心者 - Qiita](https://qiita.com/kindamu24005/items/efd53c7511a40ddac636#:~:text=%E7%89%A9%E4%BD%93%E6%A4%9C%E5%87%BA))。